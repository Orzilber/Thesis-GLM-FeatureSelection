{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17a437da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting cupy\n",
      "  Using cached cupy-10.5.0.tar.gz (1.7 MB)\n",
      "Requirement already satisfied: numpy<1.25,>=1.18 in c:\\programdata\\anaconda3\\lib\\site-packages (from cupy) (1.21.5)\n",
      "Requirement already satisfied: fastrlock>=0.5 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from cupy) (0.8)\n",
      "Building wheels for collected packages: cupy\n",
      "  Building wheel for cupy (setup.py): started\n",
      "  Building wheel for cupy (setup.py): still running...\n",
      "  Building wheel for cupy (setup.py): still running...\n",
      "  Building wheel for cupy (setup.py): still running...\n",
      "  Building wheel for cupy (setup.py): still running...\n",
      "  Building wheel for cupy (setup.py): still running...\n",
      "  Building wheel for cupy (setup.py): still running...\n",
      "  Building wheel for cupy (setup.py): still running...\n",
      "  Building wheel for cupy (setup.py): still running...\n",
      "  Building wheel for cupy (setup.py): still running...\n",
      "  Building wheel for cupy (setup.py): finished with status 'done'\n",
      "  Created wheel for cupy: filename=cupy-10.5.0-cp39-cp39-win_amd64.whl size=62096440 sha256=7972e60d4143402ef04368ad06ecabcce57662ce41b0d0c44281906f9faadc6a\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\af\\7b\\67\\b2e056d901a3b1a5b6320cf628349e5e0690e5a54b300f6155\n",
      "Successfully built cupy\n",
      "Installing collected packages: cupy\n",
      "Successfully installed cupy-10.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install cuda\n",
    "%pip install cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25c1d41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de5756e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "from scipy.stats import poisson, norm, multivariate_normal\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import multi_dot\n",
    "\n",
    "# from cupy.linalg import inv\n",
    "# from cupy.linalg import multi_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9d06a",
   "metadata": {},
   "source": [
    "# General funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f3f3180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pois_ll(X, y, beta):\n",
    "    Xb = np.dot(X,beta)\n",
    "    ll = np.dot(Xb,y) - np.sum(np.exp(Xb))\n",
    "    return ll\n",
    "\n",
    "def pois_KL(X, y, beta, theta = None):\n",
    "    Xb = np.dot(X,beta)\n",
    "    if theta is None:\n",
    "        rel = y\n",
    "    else:\n",
    "        rel = theta\n",
    "    kl_1 = np.dot(np.exp(rel),rel-Xb)\n",
    "    kl_2 = np.exp(rel) - np.exp(Xb)\n",
    "    KL = np.sum(kl_1-kl_2)\n",
    "    return KL\n",
    "\n",
    "def nb_ll(X, y, beta, alpha):\n",
    "    Xb = np.dot(X,beta)\n",
    "    ll = np.dot(y,Xb) - np.dot(np.ones(y.shape[0]) * alpha + y , np.log(np.exp(Xb) + alpha))\n",
    "    return np.sum(ll)\n",
    "\n",
    "def nb_KL(X, y, beta, alpha ,theta = None):\n",
    "    Xb = np.dot(X,beta)\n",
    "    if theta is None:\n",
    "        rel = y\n",
    "    else:\n",
    "        rel = theta\n",
    "    kl_1_par1 = np.log(np.exp(rel) / (np.exp(rel) + alpha))\n",
    "    kl_1_par2 = np.log(np.exp(Xb) / (np.exp(Xb) + alpha))\n",
    "    kl_1 = np.dot(exp(rel),kl_1_par1-kl_1_par2)\n",
    "    \n",
    "    kl_2 = np.log((np.exp(Xb) + alpha) / (np.exp(y) + alpha)) * alpha\n",
    "    \n",
    "    KL = np.sum(kl_1+kl_2)\n",
    "    return KL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4289202b",
   "metadata": {},
   "source": [
    "# IRLS and Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d77a916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#### IRLS Algorithm ####\n",
    "########################\n",
    "\n",
    "def IRLS(X, y, reg_type: ['poisson','nb']\n",
    "         , alpha = None, threshold = 0.01\n",
    "         , just_score = True):    \n",
    "    beta = np.zeros((X.shape[1]))\n",
    "    #### Distribution specifics ####\n",
    "    ## Poisson    \n",
    "    if reg_type == 'poisson':\n",
    "        ll_cur = pois_ll(X, y, beta)\n",
    "        W = np.diagflat(np.exp(np.dot(X,beta)))\n",
    "        D = np.diagflat(np.exp(np.dot(X,beta)))\n",
    "        z = np.dot(X,beta) + np.dot(inv(D),(y-np.exp(np.dot(X,beta))))\n",
    "        beta_est = multi_dot([inv(multi_dot([np.transpose(X),W,X])) , np.transpose(X), W, z])\n",
    "        ll_next = pois_ll(X, y, beta_est)\n",
    "        \n",
    "        # IRLS part\n",
    "        \n",
    "        while ll_next - ll_cur > threshold:\n",
    "            ll_cur = pois_ll(X, y, beta_est)\n",
    "            W = np.diagflat(np.exp(np.dot(X,beta_est)))\n",
    "            D = np.diagflat(np.exp(np.dot(X,beta_est)))\n",
    "            z = np.dot(X,beta_est) + np.dot(inv(D),(y-np.exp(np.dot(X,beta_est))))\n",
    "            beta_est = multi_dot([inv(multi_dot([np.transpose(X),W,X])) , np.transpose(X), W, z])\n",
    "            ll_next = pois_ll(X, y, beta_est)\n",
    "    ## NB        \n",
    "    if reg_type == 'nb':\n",
    "        ll_cur = nb_ll(X, y, beta, alpha)\n",
    "        W = np.diagflat(np.exp(np.dot(X,beta))/(1+(alpha**-1) * np.exp(np.dot(X,beta)))) # adjust to nb\n",
    "        D = np.diagflat(np.exp(np.dot(X,beta)))\n",
    "        z = np.dot(X,beta) + np.dot(inv(D),(y-np.exp(np.dot(X,beta))))\n",
    "        beta_est = np.dot(multi_dot([inv(multi_dot([np.transpose(X),W,X])) , np.transpose(X), W]), z)\n",
    "        ll_next = nb_ll(X, y, beta_est, alpha)\n",
    "        \n",
    "        # IRLS part\n",
    "        \n",
    "        while ll_next - ll_cur > threshold:\n",
    "            ll_cur = nb_ll(X, y, beta_est, alpha)\n",
    "            W = np.diagflat(np.exp(np.dot(X,beta_est))/(1+(alpha**-1) * np.exp(np.dot(X,beta_est)))) # adjust to nb\n",
    "            D = np.diagflat((np.exp(np.dot(X,beta_est))))\n",
    "            z = np.dot(X,beta_est) + np.dot(inv(D),(y-np.exp(np.dot(X,beta_est))))\n",
    "            beta_est = np.dot(multi_dot([inv(multi_dot([np.transpose(X),W,X])) , np.transpose(X), W]), z)\n",
    "            ll_next = nb_ll(X, y, beta_est, alpha)\n",
    "    \n",
    "    if just_score:\n",
    "        return ll_next\n",
    "    else:\n",
    "        return beta_est, ll_next, np.exp(np.dot(X,beta_est))\n",
    "\n",
    "###########################\n",
    "#### Forward Algorithm ####\n",
    "###########################\n",
    "\n",
    "def fwd(X, y, \n",
    "        reg_type: ['poisson','nb'], criteria: ['AIC','BIC','RIC']\n",
    "        , sel_feat = None, sel_dict = None, alpha = None):\n",
    "    \"\"\"\n",
    "    X, y, \n",
    "        reg_type: ['poisson','nb'], criteria: ['AIC','BIC','RIC']\n",
    "        , sel_feat = None, sel_dict = None, alpha = None\n",
    "    \"\"\"\n",
    "    if sel_feat is None:\n",
    "        sel_feat = []\n",
    "    if sel_dict is None:\n",
    "        sel_dict = {'features':[],'score': np.inf}\n",
    "#         feat_score =\n",
    "    for feature in [i for i in range(X.shape[1]) if i not in sel_feat]:\n",
    "#         print(sel_feat)\n",
    "#         print(feature)\n",
    "        in_feat = sel_feat+[feature]\n",
    "#         print(in_feat)\n",
    "#         print('currently testing:')\n",
    "#         print(in_feat)\n",
    "        # Criteria definition:\n",
    "        if criteria == 'AIC':\n",
    "            pen = len(in_feat)\n",
    "        if criteria == 'BIC':\n",
    "            pen = 0.5 * np.log(X.shape[0])* len(in_feat)\n",
    "        if criteria == 'RIC':\n",
    "            pen = np.log(X.shape[1])* len(in_feat)\n",
    "        if criteria == 'NLP': #NLP - Non-Linear penalty\n",
    "            pen = len(in_feat) * np.log(X.shape[1] * np.exp(1) / len(in_feat))\n",
    "        feat_score_next = -IRLS(X[:,in_feat],y,reg_type,alpha)\n",
    "        if feat_score_next < sel_dict['score']:\n",
    "            feat_score = feat_score_next + pen # taking the ll score\n",
    "            sel_dict['features'] = in_feat\n",
    "            sel_dict['score'] = feat_score\n",
    "            sel_dict['-ll']  = feat_score_next\n",
    "    return sel_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02d2370",
   "metadata": {},
   "source": [
    "# FISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a533c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "def pois_nll_grad(X,y,beta):\n",
    "    Xb = np.dot(X,beta).astype(np.float64)\n",
    "#     exp_ob = \n",
    "    nll_grad = np.dot(np.transpose(X),np.exp(Xb) - y)\n",
    "    return nll_grad\n",
    "\n",
    "def nb_nll_grad(X,y,beta,alpha):\n",
    "    Xb = np.dot(X,beta).astype(np.float64)\n",
    "    sec_factor = (np.exp(Xb) - y)/(np.exp(Xb) + alpha)\n",
    "    nll_grad = alpha * np.dot(np.transpose(X),sec_factor)\n",
    "    return nll_grad\n",
    "\n",
    "def L_pois(X,y):\n",
    "    eigs = LA.eigh(np.dot(np.transpose(X),X))[0]\n",
    "    idx = eigs.argsort()[::-1][0]  \n",
    "    eig_max = eigs[idx]\n",
    "    return np.mean(y) * eig_max\n",
    "    \n",
    "def L_nb(X,y,alpha):\n",
    "    eigs = LA.eigh(np.dot(np.transpose(X),X))[0]\n",
    "    idx = eigs.argsort()[::-1][0]  \n",
    "    eig_max = eigs[idx]\n",
    "    return (alpha + np.mean(y))/alpha * eig_max\n",
    "\n",
    "def prox(grad, beta, L, pen_vec):\n",
    "    prox_inp = beta - 1/L * grad\n",
    "    prox_out = np.maximum(np.abs(prox_inp) - pen_vec,0) * np.sign(prox_inp)\n",
    "    return prox_out\n",
    "\n",
    "def FISTA(X, y, pen_vec, \n",
    "          type: ['poisson', 'nb'],\n",
    "          iterations = 1000, is_ordered = True\n",
    "          ,alpha = None\n",
    "          ):\n",
    "    \"\"\"\n",
    "    X - Design matrix\n",
    "    y - Dependent variables\n",
    "    pen_vec - The penalty vector\n",
    "    type - The regression type - Poisson or NB\n",
    "    Iterations - Number of iterations \n",
    "    is_ordered - True\n",
    "    alpha - Only relevant if we use NB\n",
    "    \"\"\"\n",
    "    beta_start = np.zeros(X.shape[1], dtype = np.float64)\n",
    "    w_start = np.zeros(X.shape[1], dtype = np.float64)\n",
    "    delta_start = 1\n",
    "    for k in range(iterations):\n",
    "        if type == 'poisson':\n",
    "            grad = pois_nll_grad(X, y, beta_start)\n",
    "            L = L_pois(X,y)\n",
    "        elif type == 'nb':\n",
    "            grad = nb_nll_grad(X, y, beta_start,alpha)\n",
    "            L = L_nb(X,y,alpha)\n",
    "        ### Ordering for the thresholding ###\n",
    "        \n",
    "        if is_ordered:\n",
    "\n",
    "            indx = np.argsort(beta_start)[::-1]\n",
    "\n",
    "            beta_start = beta_start[indx]\n",
    "            ind_dict = {i: j for i,j in zip([*range(beta_start.shape[0])], indx)}\n",
    "        ### Starting the FISTA. Pay attention that we must sort grad according to indx\n",
    "        \n",
    "        w_next = prox(grad[indx], beta_start, L, pen_vec)\n",
    "        delta_next = (1 + np.sqrt(1 + 4*delta_start**2))/2\n",
    "        beta_next = w_next + ((delta_start - 1)/delta_next)*(w_next - w_start)\n",
    "        \n",
    "        ### Re-ordering again for calculating the gradient\n",
    "        \n",
    "        beta_start = np.zeros(beta_next.shape[0])\n",
    "        for origin_ind in ind_dict:\n",
    "            beta_start[origin_ind] = beta_next[ind_dict[origin_ind]]\n",
    "        delta_start = delta_next\n",
    "        w_start = w_next\n",
    "    \n",
    "#         if k % 100 == 0:\n",
    "#             print(k)\n",
    "#             print('beta_start: {}'.format(beta_start))\n",
    "#             print('beta_next: {}'.format(beta_next))\n",
    "#             if type == 'poisson':\n",
    "#                 print('nll :{}'.format(-pois_ll(X,y,beta_next)))\n",
    "#             elif type == 'nb':\n",
    "#                 print('nll :{}'.format(-nb_ll(X,y,beta_next)))\n",
    "    return beta_next, ind_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d327376f",
   "metadata": {},
   "source": [
    "# Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db53ae3e",
   "metadata": {},
   "source": [
    "## Simulations Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e82b608c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{20: [1, 2, 3, 4, 5, 6, 7, 10, 14, 18],\n",
       " 100: [5, 10, 15, 20, 25, 30, 35, 50, 70, 90],\n",
       " 200: [10, 20, 30, 40, 50, 60, 70, 100, 140, 180],\n",
       " 500: [10, 20, 30, 40, 50, 60, 70, 100, 140, 180],\n",
       " 1000: [10, 20, 30, 40, 50, 60, 70, 100, 140, 180]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "rho = [0,0.5,0.8]\n",
    "n = 200\n",
    "d = [20, 100, 200, 500, 1000]\n",
    "per = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.5, 0.7, 0.9]\n",
    "simulation_settings = {i: [] for i in d}\n",
    "\n",
    "def d_0(d, per, n):\n",
    "    d_0 = per*np.minimum(d,n)\n",
    "    return int(d_0)\n",
    "\n",
    "for di in d:\n",
    "#     print('d: {}'.format(di))\n",
    "    for peri in per:\n",
    "#         print('per: {}'.format(peri))\n",
    "#         print(d_0(di,peri,n))\n",
    "        simulation_settings[di].append(d_0(di,peri,n))\n",
    "# d_0(di,peri,n)\n",
    "simulation_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78548626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{20: [1, 2, 3, 4, 5, 6, 7, 10, 14, 18],\n",
       " 100: [5, 10, 15, 20, 25, 30, 35, 50, 70, 90],\n",
       " 200: [10, 20, 30, 40, 50, 60, 70, 100, 140, 180],\n",
       " 500: [10, 20, 30, 40, 50, 60, 70, 100, 140, 180],\n",
       " 1000: [10, 20, 30, 40, 50, 60, 70, 100, 140, 180]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulation_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5f982cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using penalty:AIC\n",
      "currently testing:\n",
      "[0]\n",
      "currently testing:\n",
      "[1]\n",
      "currently testing:\n",
      "[2]\n",
      "currently testing:\n",
      "[3]\n",
      "currently testing:\n",
      "[4]\n",
      "currently testing:\n",
      "[5]\n",
      "currently testing:\n",
      "[6]\n",
      "currently testing:\n",
      "[7]\n",
      "currently testing:\n",
      "[8]\n",
      "currently testing:\n",
      "[9]\n",
      "currently testing:\n",
      "[10]\n",
      "currently testing:\n",
      "[11]\n",
      "currently testing:\n",
      "[12]\n",
      "currently testing:\n",
      "[13]\n",
      "currently testing:\n",
      "[14]\n",
      "currently testing:\n",
      "[15]\n",
      "currently testing:\n",
      "[16]\n",
      "currently testing:\n",
      "[17]\n",
      "currently testing:\n",
      "[18]\n",
      "currently testing:\n",
      "[19]\n",
      "currently testing:\n",
      "[20]\n",
      "currently testing:\n",
      "[21]\n",
      "currently testing:\n",
      "[22]\n",
      "currently testing:\n",
      "[23]\n",
      "currently testing:\n",
      "[24]\n",
      "currently testing:\n",
      "[25]\n",
      "currently testing:\n",
      "[26]\n",
      "currently testing:\n",
      "[27]\n",
      "currently testing:\n",
      "[28]\n",
      "currently testing:\n",
      "[29]\n",
      "currently testing:\n",
      "[30]\n",
      "currently testing:\n",
      "[31]\n",
      "currently testing:\n",
      "[32]\n",
      "currently testing:\n",
      "[33]\n",
      "currently testing:\n",
      "[34]\n",
      "currently testing:\n",
      "[35]\n",
      "currently testing:\n",
      "[36]\n",
      "currently testing:\n",
      "[37]\n",
      "currently testing:\n",
      "[38]\n",
      "currently testing:\n",
      "[39]\n",
      "currently testing:\n",
      "[40]\n",
      "currently testing:\n",
      "[41]\n",
      "currently testing:\n",
      "[42]\n",
      "currently testing:\n",
      "[43]\n",
      "currently testing:\n",
      "[44]\n",
      "currently testing:\n",
      "[45]\n",
      "currently testing:\n",
      "[46]\n",
      "currently testing:\n",
      "[47]\n",
      "currently testing:\n",
      "[48]\n",
      "currently testing:\n",
      "[49]\n",
      "currently testing:\n",
      "[49, 0]\n",
      "currently testing:\n",
      "[49, 1]\n",
      "currently testing:\n",
      "[49, 2]\n",
      "currently testing:\n",
      "[49, 3]\n",
      "currently testing:\n",
      "[49, 4]\n",
      "currently testing:\n",
      "[49, 5]\n",
      "currently testing:\n",
      "[49, 6]\n",
      "currently testing:\n",
      "[49, 7]\n",
      "currently testing:\n",
      "[49, 8]\n",
      "currently testing:\n",
      "[49, 9]\n",
      "currently testing:\n",
      "[49, 10]\n",
      "currently testing:\n",
      "[49, 11]\n",
      "currently testing:\n",
      "[49, 12]\n",
      "currently testing:\n",
      "[49, 13]\n",
      "currently testing:\n",
      "[49, 14]\n",
      "currently testing:\n",
      "[49, 15]\n",
      "currently testing:\n",
      "[49, 16]\n",
      "currently testing:\n",
      "[49, 17]\n",
      "currently testing:\n",
      "[49, 18]\n",
      "currently testing:\n",
      "[49, 19]\n",
      "currently testing:\n",
      "[49, 20]\n",
      "currently testing:\n",
      "[49, 21]\n",
      "currently testing:\n",
      "[49, 22]\n",
      "currently testing:\n",
      "[49, 23]\n",
      "currently testing:\n",
      "[49, 24]\n",
      "currently testing:\n",
      "[49, 25]\n",
      "currently testing:\n",
      "[49, 26]\n",
      "currently testing:\n",
      "[49, 27]\n",
      "currently testing:\n",
      "[49, 28]\n",
      "currently testing:\n",
      "[49, 29]\n",
      "currently testing:\n",
      "[49, 30]\n",
      "currently testing:\n",
      "[49, 31]\n",
      "currently testing:\n",
      "[49, 32]\n",
      "currently testing:\n",
      "[49, 33]\n",
      "currently testing:\n",
      "[49, 34]\n",
      "currently testing:\n",
      "[49, 35]\n",
      "currently testing:\n",
      "[49, 36]\n",
      "currently testing:\n",
      "[49, 37]\n",
      "currently testing:\n",
      "[49, 38]\n",
      "currently testing:\n",
      "[49, 39]\n",
      "currently testing:\n",
      "[49, 40]\n",
      "currently testing:\n",
      "[49, 41]\n",
      "currently testing:\n",
      "[49, 42]\n",
      "currently testing:\n",
      "[49, 43]\n",
      "currently testing:\n",
      "[49, 44]\n",
      "currently testing:\n",
      "[49, 45]\n",
      "currently testing:\n",
      "[49, 46]\n",
      "currently testing:\n",
      "[49, 47]\n",
      "currently testing:\n",
      "[49, 48]\n",
      "using penalty:BIC\n",
      "currently testing:\n",
      "[0]\n",
      "currently testing:\n",
      "[1]\n",
      "currently testing:\n",
      "[2]\n",
      "currently testing:\n",
      "[3]\n",
      "currently testing:\n",
      "[4]\n",
      "currently testing:\n",
      "[5]\n",
      "currently testing:\n",
      "[6]\n",
      "currently testing:\n",
      "[7]\n",
      "currently testing:\n",
      "[8]\n",
      "currently testing:\n",
      "[9]\n",
      "currently testing:\n",
      "[10]\n",
      "currently testing:\n",
      "[11]\n",
      "currently testing:\n",
      "[12]\n",
      "currently testing:\n",
      "[13]\n",
      "currently testing:\n",
      "[14]\n",
      "currently testing:\n",
      "[15]\n",
      "currently testing:\n",
      "[16]\n",
      "currently testing:\n",
      "[17]\n",
      "currently testing:\n",
      "[18]\n",
      "currently testing:\n",
      "[19]\n",
      "currently testing:\n",
      "[20]\n",
      "currently testing:\n",
      "[21]\n",
      "currently testing:\n",
      "[22]\n",
      "currently testing:\n",
      "[23]\n",
      "currently testing:\n",
      "[24]\n",
      "currently testing:\n",
      "[25]\n",
      "currently testing:\n",
      "[26]\n",
      "currently testing:\n",
      "[27]\n",
      "currently testing:\n",
      "[28]\n",
      "currently testing:\n",
      "[29]\n",
      "currently testing:\n",
      "[30]\n",
      "currently testing:\n",
      "[31]\n",
      "currently testing:\n",
      "[32]\n",
      "currently testing:\n",
      "[33]\n",
      "currently testing:\n",
      "[34]\n",
      "currently testing:\n",
      "[35]\n",
      "currently testing:\n",
      "[36]\n",
      "currently testing:\n",
      "[37]\n",
      "currently testing:\n",
      "[38]\n",
      "currently testing:\n",
      "[39]\n",
      "currently testing:\n",
      "[40]\n",
      "currently testing:\n",
      "[41]\n",
      "currently testing:\n",
      "[42]\n",
      "currently testing:\n",
      "[43]\n",
      "currently testing:\n",
      "[44]\n",
      "currently testing:\n",
      "[45]\n",
      "currently testing:\n",
      "[46]\n",
      "currently testing:\n",
      "[47]\n",
      "currently testing:\n",
      "[48]\n",
      "currently testing:\n",
      "[49]\n",
      "currently testing:\n",
      "[49, 0]\n",
      "currently testing:\n",
      "[49, 1]\n",
      "currently testing:\n",
      "[49, 2]\n",
      "currently testing:\n",
      "[49, 3]\n",
      "currently testing:\n",
      "[49, 4]\n",
      "currently testing:\n",
      "[49, 5]\n",
      "currently testing:\n",
      "[49, 6]\n",
      "currently testing:\n",
      "[49, 7]\n",
      "currently testing:\n",
      "[49, 8]\n",
      "currently testing:\n",
      "[49, 9]\n",
      "currently testing:\n",
      "[49, 10]\n",
      "currently testing:\n",
      "[49, 11]\n",
      "currently testing:\n",
      "[49, 12]\n",
      "currently testing:\n",
      "[49, 13]\n",
      "currently testing:\n",
      "[49, 14]\n",
      "currently testing:\n",
      "[49, 15]\n",
      "currently testing:\n",
      "[49, 16]\n",
      "currently testing:\n",
      "[49, 17]\n",
      "currently testing:\n",
      "[49, 18]\n",
      "currently testing:\n",
      "[49, 19]\n",
      "currently testing:\n",
      "[49, 20]\n",
      "currently testing:\n",
      "[49, 21]\n",
      "currently testing:\n",
      "[49, 22]\n",
      "currently testing:\n",
      "[49, 23]\n",
      "currently testing:\n",
      "[49, 24]\n",
      "currently testing:\n",
      "[49, 25]\n",
      "currently testing:\n",
      "[49, 26]\n",
      "currently testing:\n",
      "[49, 27]\n",
      "currently testing:\n",
      "[49, 28]\n",
      "currently testing:\n",
      "[49, 29]\n",
      "currently testing:\n",
      "[49, 30]\n",
      "currently testing:\n",
      "[49, 31]\n",
      "currently testing:\n",
      "[49, 32]\n",
      "currently testing:\n",
      "[49, 33]\n",
      "currently testing:\n",
      "[49, 34]\n",
      "currently testing:\n",
      "[49, 35]\n",
      "currently testing:\n",
      "[49, 36]\n",
      "currently testing:\n",
      "[49, 37]\n",
      "currently testing:\n",
      "[49, 38]\n",
      "currently testing:\n",
      "[49, 39]\n",
      "currently testing:\n",
      "[49, 40]\n",
      "currently testing:\n",
      "[49, 41]\n",
      "currently testing:\n",
      "[49, 42]\n",
      "currently testing:\n",
      "[49, 43]\n",
      "currently testing:\n",
      "[49, 44]\n",
      "currently testing:\n",
      "[49, 45]\n",
      "currently testing:\n",
      "[49, 46]\n",
      "currently testing:\n",
      "[49, 47]\n",
      "currently testing:\n",
      "[49, 48]\n",
      "using penalty:RIC\n",
      "currently testing:\n",
      "[0]\n",
      "currently testing:\n",
      "[1]\n",
      "currently testing:\n",
      "[2]\n",
      "currently testing:\n",
      "[3]\n",
      "currently testing:\n",
      "[4]\n",
      "currently testing:\n",
      "[5]\n",
      "currently testing:\n",
      "[6]\n",
      "currently testing:\n",
      "[7]\n",
      "currently testing:\n",
      "[8]\n",
      "currently testing:\n",
      "[9]\n",
      "currently testing:\n",
      "[10]\n",
      "currently testing:\n",
      "[11]\n",
      "currently testing:\n",
      "[12]\n",
      "currently testing:\n",
      "[13]\n",
      "currently testing:\n",
      "[14]\n",
      "currently testing:\n",
      "[15]\n",
      "currently testing:\n",
      "[16]\n",
      "currently testing:\n",
      "[17]\n",
      "currently testing:\n",
      "[18]\n",
      "currently testing:\n",
      "[19]\n",
      "currently testing:\n",
      "[20]\n",
      "currently testing:\n",
      "[21]\n",
      "currently testing:\n",
      "[22]\n",
      "currently testing:\n",
      "[23]\n",
      "currently testing:\n",
      "[24]\n",
      "currently testing:\n",
      "[25]\n",
      "currently testing:\n",
      "[26]\n",
      "currently testing:\n",
      "[27]\n",
      "currently testing:\n",
      "[28]\n",
      "currently testing:\n",
      "[29]\n",
      "currently testing:\n",
      "[30]\n",
      "currently testing:\n",
      "[31]\n",
      "currently testing:\n",
      "[32]\n",
      "currently testing:\n",
      "[33]\n",
      "currently testing:\n",
      "[34]\n",
      "currently testing:\n",
      "[35]\n",
      "currently testing:\n",
      "[36]\n",
      "currently testing:\n",
      "[37]\n",
      "currently testing:\n",
      "[38]\n",
      "currently testing:\n",
      "[39]\n",
      "currently testing:\n",
      "[40]\n",
      "currently testing:\n",
      "[41]\n",
      "currently testing:\n",
      "[42]\n",
      "currently testing:\n",
      "[43]\n",
      "currently testing:\n",
      "[44]\n",
      "currently testing:\n",
      "[45]\n",
      "currently testing:\n",
      "[46]\n",
      "currently testing:\n",
      "[47]\n",
      "currently testing:\n",
      "[48]\n",
      "currently testing:\n",
      "[49]\n",
      "currently testing:\n",
      "[49, 0]\n",
      "currently testing:\n",
      "[49, 1]\n",
      "currently testing:\n",
      "[49, 2]\n",
      "currently testing:\n",
      "[49, 3]\n",
      "currently testing:\n",
      "[49, 4]\n",
      "currently testing:\n",
      "[49, 5]\n",
      "currently testing:\n",
      "[49, 6]\n",
      "currently testing:\n",
      "[49, 7]\n",
      "currently testing:\n",
      "[49, 8]\n",
      "currently testing:\n",
      "[49, 9]\n",
      "currently testing:\n",
      "[49, 10]\n",
      "currently testing:\n",
      "[49, 11]\n",
      "currently testing:\n",
      "[49, 12]\n",
      "currently testing:\n",
      "[49, 13]\n",
      "currently testing:\n",
      "[49, 14]\n",
      "currently testing:\n",
      "[49, 15]\n",
      "currently testing:\n",
      "[49, 16]\n",
      "currently testing:\n",
      "[49, 17]\n",
      "currently testing:\n",
      "[49, 18]\n",
      "currently testing:\n",
      "[49, 19]\n",
      "currently testing:\n",
      "[49, 20]\n",
      "currently testing:\n",
      "[49, 21]\n",
      "currently testing:\n",
      "[49, 22]\n",
      "currently testing:\n",
      "[49, 23]\n",
      "currently testing:\n",
      "[49, 24]\n",
      "currently testing:\n",
      "[49, 25]\n",
      "currently testing:\n",
      "[49, 26]\n",
      "currently testing:\n",
      "[49, 27]\n",
      "currently testing:\n",
      "[49, 28]\n",
      "currently testing:\n",
      "[49, 29]\n",
      "currently testing:\n",
      "[49, 30]\n",
      "currently testing:\n",
      "[49, 31]\n",
      "currently testing:\n",
      "[49, 32]\n",
      "currently testing:\n",
      "[49, 33]\n",
      "currently testing:\n",
      "[49, 34]\n",
      "currently testing:\n",
      "[49, 35]\n",
      "currently testing:\n",
      "[49, 36]\n",
      "currently testing:\n",
      "[49, 37]\n",
      "currently testing:\n",
      "[49, 38]\n",
      "currently testing:\n",
      "[49, 39]\n",
      "currently testing:\n",
      "[49, 40]\n",
      "currently testing:\n",
      "[49, 41]\n",
      "currently testing:\n",
      "[49, 42]\n",
      "currently testing:\n",
      "[49, 43]\n",
      "currently testing:\n",
      "[49, 44]\n",
      "currently testing:\n",
      "[49, 45]\n",
      "currently testing:\n",
      "[49, 46]\n",
      "currently testing:\n",
      "[49, 47]\n",
      "currently testing:\n",
      "[49, 48]\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "score_start = np.inf\n",
    "final_scores = {'AIC': {}, 'BIC': {} ,'RIC': {}}\n",
    "for crit in ['AIC','BIC','RIC']:\n",
    "    print('using penalty:{}'.format(crit))\n",
    "#     criteria = crit\n",
    "    score_start = np.inf\n",
    "    sel_dict = fwd(X[:300,:],y[:300],'poisson', crit)\n",
    "#     print(sel_dict)\n",
    "    counter = 0\n",
    "    while score_start > sel_dict['score']:\n",
    "        counter +=1\n",
    "        score_start = sel_dict['score']\n",
    "        sel_dict = fwd(X[:300,:],y[:300],'poisson',crit, sel_feat = sel_dict['features'], sel_dict = sel_dict)\n",
    "#         print('finished')\n",
    "#         print(counter)\n",
    "#     print('Results')\n",
    "#     print(sel_dict)\n",
    "    final_scores[crit] = sel_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2be0c930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AIC',\n",
       "  {'features': [49, 48],\n",
       "   'score': 303.6656246224452,\n",
       "   '-ll': 299.6656246224452}),\n",
       " ('BIC',\n",
       "  {'features': [49, 48],\n",
       "   'score': 311.0731895717576,\n",
       "   '-ll': 299.6656246224452}),\n",
       " ('RIC',\n",
       "  {'features': [49, 48],\n",
       "   'score': 315.31371664415775,\n",
       "   '-ll': 299.6656246224452})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_scores.items()\n",
    "sorted(final_scores.items(), key=lambda item: item[1]['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc14845d",
   "metadata": {},
   "source": [
    "## Simluations Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c993bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "from numpy.random import multivariate_normal\n",
    "import multiprocessing\n",
    "import itertools\n",
    "\n",
    "##### What is left?\n",
    "#### 1. KL - Over y test and Theta\n",
    "#### 2. Model Size\n",
    "\n",
    "def runner(X, y, theta\n",
    "           , reg_type: ['poisson','nb']\n",
    "#            , model_type: ['fwd','LASSO','SLOPE']\n",
    "           , pen_coef = np.exp(np.arange(-40,40,0.3))):\n",
    "    \n",
    "    # Creating train and test sets\n",
    "    d = X.shape[1]\n",
    "    (X_train, y_train, theta_train), (X_test, y_test, theta_test) = train_test_allocator(X, y, theta)\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    \n",
    "    model_scores_dict = {'FWD':{'nll':0,'KL':0, 'KL_theta_train':0, 'KL_theta_test':0, 'size':0}\n",
    "                         ,'LASSO':{'nll':0,'KL':0, 'KL_theta_train':0, 'KL_theta_test':0, 'size':0}\n",
    "                         ,'SLOPE':{'nll':0,'KL':0, 'KL_theta_train':0, 'KL_theta_test':0, 'size':0}}\n",
    "    # Creating k-folds #\n",
    "    indices = np.array([*range(X_train.shape[0])])\n",
    "    k_folds_inds = np.split(indices,5)\n",
    "    \n",
    "    #####################\n",
    "    # Forward selection #\n",
    "    #####################\n",
    "    print('Starting Forward Selection')\n",
    "    \n",
    "#     final_scores = {'AIC': {}, 'BIC': {} ,'RIC': {}}\n",
    "    folds_score = {'AIC': 0, 'BIC': 0 ,'RIC': 0, 'NLP':0}\n",
    "    # K-fold part\n",
    "\n",
    "    for crit in ['AIC','BIC','RIC','NLP']:\n",
    "        print('using penalty:{}'.format(crit))\n",
    "        score_start = np.inf\n",
    "#         folds_score = {'AIC': 0, 'BIC': 0 ,'RIC': 0}\n",
    "        for oos_fold in k_folds_inds:\n",
    "            fit_folds = [i for i in indices if i not in oos_fold]\n",
    "            # val set\n",
    "            val_set = X_train[oos_fold,:]\n",
    "            val_y = y_train[oos_fold]\n",
    "            # fit set\n",
    "            fit_set = X_train[fit_folds,:]        \n",
    "            fit_y = y_train[fit_folds]\n",
    "            sel_dict = fwd(X = fit_set,y = fit_y,reg_type = reg_type, criteria = crit)\n",
    "            while score_start > sel_dict['score']:\n",
    "#                 counter +=1\n",
    "                score_start = sel_dict['score']\n",
    "                sel_dict = fwd(fit_set,fit_y, reg_type,crit\n",
    "                               , sel_feat = sel_dict['features'], sel_dict = sel_dict)\n",
    "            \n",
    "            selected_features = sel_dict['features']\n",
    "            beta_est_vals = IRLS(fit_set[:,selected_features], fit_y, reg_type, just_score = False)[0]\n",
    "            beta_est = np.zeros(d)\n",
    "            beta_est[selected_features] = beta_est_vals\n",
    "            print(beta_est)\n",
    "            if reg_type == 'poisson':\n",
    "                ll = pois_ll(val_set, val_y, beta_est)\n",
    "            elif reg_type == 'nb':\n",
    "                ll = nb_ll(val_set, val_y\n",
    "                             , beta_est, alpha)\n",
    "            folds_score[crit] += ll\n",
    "            print(folds_score)\n",
    "        folds_score[crit] = folds_score[crit]/5\n",
    "#         final_scores[crit] = folds_score[crit]\n",
    "    \n",
    "    # Taking the best option:\n",
    "    \n",
    "    print(sorted(folds_score.items(), key=lambda item: item))\n",
    "    best_score_crit = sorted(folds_score.items(), key=lambda item: item[1])[0][0]\n",
    "    print('cv selection: {}'.format(best_score_crit))\n",
    "    # Fitting over entire train set:\n",
    "    \n",
    "    score_start = np.inf\n",
    "    sel_dict = fwd(X_train, y_train, reg_type, best_score_crit)\n",
    "#     counter = 0\n",
    "    while score_start > sel_dict['score']:\n",
    "#         counter +=1\n",
    "        score_start = sel_dict['score']\n",
    "        sel_dict = fwd(X_train, y_train, reg_type, best_score_crit\n",
    "                       , sel_feat = sel_dict['features'], sel_dict = sel_dict)\n",
    "    final_feautre_set = sel_dict['features']\n",
    "    \n",
    "    final_beta_vals = IRLS(X_train[:,final_feautre_set], y_train, reg_type, just_score = False)[0]\n",
    "    final_beta_est = np.zeros(d)\n",
    "    final_beta_est[final_feautre_set] = final_beta_vals\n",
    "    # Testing over the test set:\n",
    "    \n",
    "    if reg_type == 'poisson':\n",
    "        nll = -pois_ll( X_test, y_test, final_beta_est)\n",
    "        KL = pois_KL( X_test, y_test, final_beta_est, theta = None)\n",
    "        KL_theta_train = pois_KL( X_train, y_train, final_beta_est, theta = theta_train)\n",
    "        KL_theta_test = pois_KL( X_test, y_test, final_beta_est, theta = theta_test)\n",
    "    elif reg_type == 'nb':\n",
    "        nll = -nb_ll( X_test, y_test\n",
    "                     , final_beta_est, alpha)\n",
    "        KL = nb_KL( X_test, y_test\n",
    "                     , final_beta_est, alpha, theta = None)\n",
    "        KL_theta_train = nb_KL( X_train, y_train\n",
    "                     , final_beta_est, alpha, theta = theta_train)\n",
    "        KL_theta_test = nb_KL( X_test, y_test\n",
    "                     , final_beta_est, alpha, theta = theta_test)\n",
    "    \n",
    "    model_scores_dict['FWD']['nll'] += nll\n",
    "    model_scores_dict['FWD']['KL'] += KL\n",
    "    model_scores_dict['FWD']['KL_theta_train'] += KL_theta_train\n",
    "    model_scores_dict['FWD']['KL_theta_test'] += KL_theta_test\n",
    "    model_scores_dict['FWD']['size'] += len(final_feautre_set)\n",
    "   \n",
    "    #####################\n",
    "    ### LASSO & SLOPE ###\n",
    "    #####################\n",
    "    \n",
    "    print(\"Starting LASSO & SLOPE\")\n",
    "    \n",
    "#     d = X.shape[1]\n",
    "    pen_vec_LASSO = np.ones(d) * np.sqrt(2 * np.log(d))\n",
    "    print(pen_vec_LASSO)\n",
    "    pen_vec_SLOPE = np.array([np.sqrt(np.log(2*d/(j+1))) for j in range(d)])\n",
    "    print(pen_vec_SLOPE)\n",
    "    \n",
    "    \n",
    "    pen_coef_results = {'SLOPE':{},'LASSO':{}}\n",
    "    for C in pen_coef:\n",
    "        SLOPE_cv_score = 0\n",
    "        LASSO_cv_score = 0\n",
    "        print('testing C: {}'.format(C))\n",
    "        for oos_fold in k_folds_inds:\n",
    "            fit_folds = [i for i in indices if i not in oos_fold]\n",
    "            # val set\n",
    "            val_set = X_train[oos_fold,:]\n",
    "            val_y = y_train[oos_fold]\n",
    "            # fit set\n",
    "            fit_set = X_train[fit_folds,:]        \n",
    "            fit_y = y_train[fit_folds]\n",
    "            if reg_type == 'poisson':\n",
    "                # SLOPE\n",
    "                SLOPE_cv_beta = FISTA(fit_set,fit_y,C*pen_vec_SLOPE,reg_type)[0]\n",
    "                SLOPE_cv_score += -pois_ll(val_set, val_y, SLOPE_cv_beta)\n",
    "                \n",
    "                # LASSO\n",
    "                LASSO_cv_beta = FISTA(fit_set,fit_y,C*pen_vec_LASSO,reg_type)[0]\n",
    "                LASSO_cv_score += -pois_ll(val_set, val_y, LASSO_cv_beta)\n",
    "                \n",
    "            elif reg_type == 'nb':\n",
    "                # SLOPE\n",
    "                SLOPE_cv_beta = FISTA(fit_set,fit_y,C*pen_vec_SLOPE,reg_type, alpha = alpha)[0]\n",
    "                SLOPE_cv_score += -nb_ll(val_set, val_y, SLOPE_cv_beta, alpha = alpha)\n",
    "                # LASSO\n",
    "                LASSO_cv_beta = FISTA(fit_set,fit_y,C*pen_vec_LASSO,reg_type, alpha = alpha)[0]\n",
    "                LASSO_cv_score += -nb_ll(val_set, val_y, LASSO_cv_beta, alpha = alpha)\n",
    "        \n",
    "        pen_coef_results['SLOPE'][C] = SLOPE_cv_score/5\n",
    "        pen_coef_results['LASSO'][C] = LASSO_cv_score/5\n",
    "    \n",
    "    # Finding the best penalty value\n",
    "    \n",
    "    # Eliminating nans #\n",
    "    \n",
    "    pen_coef_results['SLOPE'] = {pen: score for pen, score in pen_coef_results['SLOPE'].items() if np.isnan(score) == False}\n",
    "    pen_coef_results['LASSO'] = {pen: score for pen, score in pen_coef_results['LASSO'].items() if np.isnan(score) == False}\n",
    "        \n",
    "    print('SLOPE result')\n",
    "    print(pen_coef_results['SLOPE'].items())\n",
    "    print({key: val for key, val in sorted(pen_coef_results['SLOPE'].items(), key=lambda item: item[1])})\n",
    "    print('LASSO result')\n",
    "    print(pen_coef_results['LASSO'].items())\n",
    "    print({key: val for key, val in sorted(pen_coef_results['LASSO'].items(), key=lambda item: item[1])})\n",
    "    SLOPE_sel_pen = sorted(pen_coef_results['SLOPE'].items(), key=lambda item: item[1])[0][0]\n",
    "    LASSO_sel_pen = sorted(pen_coef_results['LASSO'].items(), key=lambda item: item[1])[0][0]\n",
    "    \n",
    "    \n",
    "    # Fitting and testing over the best penalty value:\n",
    "    if reg_type == 'poisson':\n",
    "#         Fit\n",
    "        SLOPE_final_beta = FISTA(fit_set,fit_y,SLOPE_sel_pen*pen_vec_SLOPE,reg_type)[0]\n",
    "        SLOPE_final_beta_size = len(SLOPE_final_beta[SLOPE_final_beta > 0])\n",
    "        \n",
    "        LASSO_final_beta = FISTA(fit_set,fit_y,LASSO_sel_pen*pen_vec_LASSO,reg_type)[0]\n",
    "        LASSO_final_beta_size = len(LASSO_final_beta[LASSO_final_beta > 0])\n",
    "        \n",
    "#        eval SLOPE\n",
    "        SLOPE_nll = -pois_ll( X_test, y_test, SLOPE_final_beta)\n",
    "        SLOPE_KL = pois_KL( X_test, y_test, SLOPE_final_beta, theta = None)\n",
    "        SLOPE_KL_theta_train = pois_KL( X_train, y_train, SLOPE_final_beta, theta = theta_train)\n",
    "        SLOPE_KL_theta_test = pois_KL( X_test, y_test, SLOPE_final_beta, theta = theta_test)\n",
    "        \n",
    "#         eval LASSO\n",
    "        LASSO_nll = -pois_ll( X_test, y_test, LASSO_final_beta)\n",
    "        LASSO_KL = pois_KL( X_test, y_test, LASSO_final_beta, theta = None)\n",
    "        LASSO_KL_theta_train = pois_KL(X_train, y_train, LASSO_final_beta, theta = theta_train)\n",
    "        LASSO_KL_theta_test = pois_KL(X_test, y_test, LASSO_final_beta, theta = theta_test)\n",
    "    \n",
    "    elif reg_type == 'nb':\n",
    "#         Fit\n",
    "        SLOPE_final_beta = FISTA(fit_set,fit_y,SLOPE_sel_pen*pen_vec_SLOPE,reg_type, alpha = alpha)[0]\n",
    "        LASSO_final_beta = FISTA(fit_set,fit_y,LASSO_sel_pen*pen_vec_LASSO,reg_type, alpha = alpha)[0]\n",
    "#        eval\n",
    "        #        eval SLOPE\n",
    "        SLOPE_nll = -nb_ll( X_test, y_test, SLOPE_final_beta)\n",
    "        SLOPE_KL = nb_KL( X_test, y_test, SLOPE_final_beta, theta = None)\n",
    "        SLOPE_KL_theta_train = nb_KL( X_train, y_train, SLOPE_final_beta, theta = theta_train)\n",
    "        SLOPE_KL_theta_test = pois_KL( X_test, y_test, SLOPE_final_beta, theta = theta_test)\n",
    "        \n",
    "#         eval LASSO\n",
    "        LASSO_nll = -nb_ll( X_test, y_test, LASSO_final_beta)\n",
    "        LASSO_KL = nb_KL( X_test, y_test, LASSO_final_beta, theta = None)\n",
    "        LASSO_KL_theta_train = nb_KL( X_train, y_train, LASSO_final_beta, theta = theta_train)\n",
    "        LASSO_KL_theta_test = nb_KL( X_test, y_test, LASSO_final_beta, theta = theta_test)\n",
    "    \n",
    "    \n",
    "    model_scores_dict['SLOPE']['nll'] += SLOPE_nll\n",
    "    model_scores_dict['SLOPE']['KL'] += SLOPE_KL\n",
    "    model_scores_dict['SLOPE']['KL_theta_train'] += SLOPE_KL_theta_train\n",
    "    model_scores_dict['SLOPE']['KL_theta_test'] += SLOPE_KL_theta_test\n",
    "    model_scores_dict['SLOPE']['size'] += SLOPE_final_beta_size\n",
    "    \n",
    "    model_scores_dict['LASSO']['nll'] += LASSO_nll\n",
    "    model_scores_dict['LASSO']['KL'] += LASSO_KL\n",
    "    model_scores_dict['LASSO']['KL_theta_train'] += LASSO_KL_theta_train\n",
    "    model_scores_dict['LASSO']['KL_theta_test'] += LASSO_KL_theta_test\n",
    "    model_scores_dict['LASSO']['size'] += LASSO_final_beta_size\n",
    "    \n",
    "    print('Finished all')\n",
    "    \n",
    "    return model_scores_dict\n",
    "\n",
    "def train_test_allocator(X, y, theta, n_test = 100):\n",
    "    X_inds = [*range(X.shape[0])]\n",
    "    test_sample = np.random.choice(X_inds, 100,replace = False)\n",
    "    train_sample = [i for i in X_inds if i not in test_sample]\n",
    "    train_set = (X[train_sample], y[train_sample], theta[train_sample])\n",
    "    test_set = (X[test_sample], y[test_sample], theta[test_sample])\n",
    "    return train_set, test_set\n",
    "\n",
    "def matrix_simulator(d, d0\n",
    "                     , rho\n",
    "                     , beta_set = [0.5, -0.5, 0.6, -0.6]\n",
    "                     , n = 300\n",
    "                     , sim_num = 100):\n",
    "    \n",
    "    means = np.zeros(d)\n",
    "    if rho == 0:\n",
    "        conv_mat = np.diagflat(np.ones(d))\n",
    "    else:\n",
    "        conv_mat = cov_creator(d, rho)\n",
    "    X = multivariate_normal(means, conv_mat, size = n*sim_num)\n",
    "    X = X/norm(X, axis = 0)\n",
    "    \n",
    "    beta = beta_creator(d, d0, beta_set)\n",
    "    theta = np.exp(np.dot(X,beta))\n",
    "    y = poisson.rvs(theta) \n",
    "    return X, y, theta\n",
    "    \n",
    "def cov_creator(d, rho):\n",
    "    arr = np.zeros(d)\n",
    "    for i in range(d):\n",
    "        arr[i] = rho**((i+1)-1)\n",
    "    cov = np.zeros((d,d))\n",
    "    for i in range(d):\n",
    "        cov[i,:] = np.concatenate([arr[:i+1][::-1],arr[1:d-i]])\n",
    "    return cov\n",
    "\n",
    "def beta_creator(d, d0, beta_set = [0.5, -0.5, 0.6, -0.6]):\n",
    "    b_d0 = np.random.choice(beta_set, d)\n",
    "    zero_inds = np.random.choice([*range(d)],d-d0,replace = False)\n",
    "    b_d0[zero_inds] = 0\n",
    "    return b_d0\n",
    "\n",
    "    \n",
    "# def mp_apply():\n",
    "# for d in simulation_settings:\n",
    "#     # Simulate 100 matrices of 300 X d for each d_0\n",
    "#     # And generate the dpendent variable\n",
    "#     np."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2736ee9b",
   "metadata": {},
   "source": [
    "## Simulation Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c06d963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<multiprocessing.pool.Pool state=RUN pool_size=8>\n",
      "running d0: 1\n",
      "running rho: 0\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# pool = multiprocessing.Pool(os.cpu_count())\n",
    "# print(pool)\n",
    "# if __name__ == '__main__':\n",
    "# # output = process_pool.starmap(f_sum, data)\n",
    "# # results = {}\n",
    "# # # for d in simulation_settings:\n",
    "#     d = 20\n",
    "#     for di in simulation_settings[d][:1]:\n",
    "#         print('running d0: {}'.format(di))\n",
    "#         for rho in [0, 0.5, 0.8][:1]:\n",
    "#             print('running rho: {}'.format(rho))\n",
    "#             data = matrix_simulator(d, di, rho = rho, sim_num = 8)\n",
    "#     #             print(data)\n",
    "#             X_split = np.vsplit(data[0], 8)\n",
    "#             y_split = np.split(data[1], 8)\n",
    "#             theta_split = np.split(data[2], 8)\n",
    "#             map_args = [(X,y,theta, 'poisson') for X,y,theta in zip(X_split,y_split,theta_split)]\n",
    "#     #         print(map_args[0])\n",
    "# #             output = pool.starmap(runner, map_args)\n",
    "#             output = pool.map(runner, map_args)\n",
    "#             print(\"Finished Pooling\")\n",
    "        \n",
    "# #             print(zip(X_split,y_split,theta_split))\n",
    "# #         result_forward = pool.starmap()\n",
    "# #     print(d, simulation_settings[d])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df07b94",
   "metadata": {},
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8f4b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('sim_dict_0', 'rb') as f:\n",
    "    results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b2f7f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(20, 1, 0), (20, 2, 0), (20, 3, 0), (20, 4, 0), (20, 5, 0), (20, 6, 0), (20, 7, 0), (20, 10, 0), (20, 14, 0), (20, 18, 0), (100, 5, 0), (100, 10, 0), (100, 15, 0), (100, 20, 0), (100, 25, 0), (100, 30, 0), (100, 35, 0), (100, 50, 0), (100, 70, 0), (100, 90, 0)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b65dc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running d: 20\n",
      "(20, 1, 0) In dictionary\n",
      "(20, 2, 0) In dictionary\n",
      "(20, 3, 0) In dictionary\n",
      "(20, 4, 0) In dictionary\n",
      "(20, 5, 0) In dictionary\n",
      "(20, 6, 0) In dictionary\n",
      "(20, 7, 0) In dictionary\n",
      "(20, 10, 0) In dictionary\n",
      "(20, 14, 0) In dictionary\n",
      "(20, 18, 0) In dictionary\n",
      "running d: 100\n",
      "(100, 5, 0) In dictionary\n",
      "(100, 10, 0) In dictionary\n",
      "(100, 15, 0) In dictionary\n",
      "(100, 20, 0) In dictionary\n",
      "(100, 25, 0) In dictionary\n",
      "(100, 30, 0) In dictionary\n",
      "(100, 35, 0) In dictionary\n",
      "(100, 50, 0) In dictionary\n",
      "(100, 70, 0) In dictionary\n",
      "(100, 90, 0) In dictionary\n",
      "running d: 200\n",
      "running d0: 10\n",
      "running rho: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import itertools\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from numpy.random import multivariate_normal, poisson\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import multi_dot\n",
    "from simulation_runner import runner as sim_runner\n",
    "from simulation_runner import matrix_simulator\n",
    "\n",
    "\n",
    "\n",
    "pool = multiprocessing.Pool(os.cpu_count())\n",
    "# print(pool)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "# output = process_pool.starmap(f_sum, data)\n",
    "#     results = {}\n",
    "    with open('sim_dict_0', 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "    for rho in [0, 0.5, 0.8]:\n",
    "        for d in simulation_settings:\n",
    "    #     d = 20\n",
    "            print('running d: {}'.format(d))\n",
    "            for di in simulation_settings[d]:\n",
    "                if (d,di,rho) in results:\n",
    "                    print('{} In dictionary'.format((d,di,rho)))\n",
    "                    continue\n",
    "                print('running d0: {}'.format(di))\n",
    "#                 for rho in [0, 0.5, 0.8]:\n",
    "                print('running rho: {}'.format(rho))\n",
    "                data = matrix_simulator(d, di, rho = rho)\n",
    "        #             print(data)\n",
    "                X_split = np.vsplit(data[0], 100)\n",
    "                y_split = np.split(data[1], 100)\n",
    "                theta_split = np.split(data[2], 100)\n",
    "                map_args = [(X,y,theta, 'poisson') for X,y,theta in zip(X_split,y_split,theta_split)]\n",
    "        #         print(map_args[0])\n",
    "                output = pool.starmap(sim_runner, map_args)\n",
    "                results[(d,di,rho)] = output\n",
    "                print('finished simulations of: {}'.format(d,di,rho))\n",
    "                with open('sim_dict_{}'.format(rho), 'wb') as fp:\n",
    "                    pickle.dump(results, fp)\n",
    "    #             output = pool.map(sim_runner, map_args)\n",
    "        print(\"Finished Pooling rho: {}\".format(rho))\n",
    "            \n",
    "        \n",
    "#             print(zip(X_split,y_split,theta_split))\n",
    "#         result_forward = pool.starmap()\n",
    "#     print(d, simulation_settings[d])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc2acaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(20,\n",
       "  1,\n",
       "  0): [{'FWD': {'nll': 102.10509670783853,\n",
       "    'KL': 84550.20936529551,\n",
       "    'KL_theta_train': 107450.24894538826,\n",
       "    'KL_theta_test': 26587.98492985053,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 84154.04825736102,\n",
       "    'KL_theta_train': 108359.25424959138,\n",
       "    'KL_theta_test': 27042.39315600711,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 84154.04825736102,\n",
       "    'KL_theta_train': 108359.25424959138,\n",
       "    'KL_theta_test': 27042.39315600711,\n",
       "    'size': 0}}, {'FWD': {'nll': 99.91478243342443,\n",
       "    'KL': 113212.54035226884,\n",
       "    'KL_theta_train': 108483.52054739305,\n",
       "    'KL_theta_test': 27015.611278017594,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 113284.0597263231,\n",
       "    'KL_theta_train': 108418.15895832314,\n",
       "    'KL_theta_test': 27013.426456544214,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 113284.0597263231,\n",
       "    'KL_theta_train': 108418.15895832314,\n",
       "    'KL_theta_test': 27013.426456544214,\n",
       "    'size': 0}}, {'FWD': {'nll': 100.34279913889554,\n",
       "    'KL': 90118.64871215801,\n",
       "    'KL_theta_train': 108635.07531096537,\n",
       "    'KL_theta_test': 27253.1750194703,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 90402.35488790977,\n",
       "    'KL_theta_train': 108335.68767033973,\n",
       "    'KL_theta_test': 27029.15910827657,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 90402.35488790977,\n",
       "    'KL_theta_train': 108335.68767033973,\n",
       "    'KL_theta_test': 27029.15910827657,\n",
       "    'size': 0}}, {'FWD': {'nll': 101.70151648797653,\n",
       "    'KL': 111731.55178996561,\n",
       "    'KL_theta_train': 106158.2346228904,\n",
       "    'KL_theta_test': 27438.008428730944,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 99.73596582598435,\n",
       "    'KL': 110914.93927560393,\n",
       "    'KL_theta_train': 108801.55433640032,\n",
       "    'KL_theta_test': 27445.828507068054,\n",
       "    'size': 4},\n",
       "   'SLOPE': {'nll': 101.34471883979266,\n",
       "    'KL': 110496.66535705196,\n",
       "    'KL_theta_train': 108709.15794225445,\n",
       "    'KL_theta_test': 26847.69854961103,\n",
       "    'size': 1}}, {'FWD': {'nll': 102.00798042861136,\n",
       "    'KL': 179557.02693261867,\n",
       "    'KL_theta_train': 108783.78980849124,\n",
       "    'KL_theta_test': 26958.447314372843,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 177732.71379046646,\n",
       "    'KL_theta_train': 108412.44826660425,\n",
       "    'KL_theta_test': 27002.601774144623,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 99.98635443780427,\n",
       "    'KL': 177748.78586923957,\n",
       "    'KL_theta_train': 108461.26763855052,\n",
       "    'KL_theta_test': 26993.217520886108,\n",
       "    'size': 0}}, {'FWD': {'nll': 103.73208555264802,\n",
       "    'KL': 110909.74485696535,\n",
       "    'KL_theta_train': 107022.44600735139,\n",
       "    'KL_theta_test': 26863.11068268011,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 115.12980245306312,\n",
       "    'KL': 114027.55293592592,\n",
       "    'KL_theta_train': 112901.87949391163,\n",
       "    'KL_theta_test': 26897.659077380937,\n",
       "    'size': 8},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 109410.01303729639,\n",
       "    'KL_theta_train': 108371.17553987377,\n",
       "    'KL_theta_test': 27040.762462965013,\n",
       "    'size': 0}}, {'FWD': {'nll': 101.06400141955159,\n",
       "    'KL': 170809.54747236928,\n",
       "    'KL_theta_train': 108415.86901068027,\n",
       "    'KL_theta_test': 27051.64711997818,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 101.2909031571778,\n",
       "    'KL': 170411.31239733938,\n",
       "    'KL_theta_train': 108929.31929491463,\n",
       "    'KL_theta_test': 27126.509037086096,\n",
       "    'size': 3},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 169323.48527172516,\n",
       "    'KL_theta_train': 108419.08782713435,\n",
       "    'KL_theta_test': 27024.331023346753,\n",
       "    'size': 0}}, {'FWD': {'nll': 101.36955126009148,\n",
       "    'KL': 154579.42643779583,\n",
       "    'KL_theta_train': 107814.58838979289,\n",
       "    'KL_theta_test': 27063.40218452909,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 152193.86367013084,\n",
       "    'KL_theta_train': 108361.60333524953,\n",
       "    'KL_theta_test': 27008.56539149656,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 108.74772667081575,\n",
       "    'KL': 158325.2999414272,\n",
       "    'KL_theta_train': 106733.59778030316,\n",
       "    'KL_theta_test': 25580.56211383215,\n",
       "    'size': 11}}, {'FWD': {'nll': 100.15282172848671,\n",
       "    'KL': 116745.6677962809,\n",
       "    'KL_theta_train': 107918.00660224917,\n",
       "    'KL_theta_test': 26989.54111149705,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 116617.90083999927,\n",
       "    'KL_theta_train': 108389.2933451336,\n",
       "    'KL_theta_test': 26984.140664944956,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 116617.90083999927,\n",
       "    'KL_theta_train': 108389.2933451336,\n",
       "    'KL_theta_test': 26984.140664944956,\n",
       "    'size': 0}}, {'FWD': {'nll': 102.62552612475041,\n",
       "    'KL': 216423.61657739396,\n",
       "    'KL_theta_train': 108428.70435299329,\n",
       "    'KL_theta_test': 27111.341585805414,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 99.8420488846346,\n",
       "    'KL': 215612.27714679396,\n",
       "    'KL_theta_train': 108582.59047320191,\n",
       "    'KL_theta_test': 26992.69451985746,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 99.83792383008436,\n",
       "    'KL': 215604.14604022988,\n",
       "    'KL_theta_train': 108585.89687994632,\n",
       "    'KL_theta_test': 26991.879982382445,\n",
       "    'size': 0}}, {'FWD': {'nll': 99.7021593410307,\n",
       "    'KL': 66148.8427857566,\n",
       "    'KL_theta_train': 110795.80069694301,\n",
       "    'KL_theta_test': 28430.867928947555,\n",
       "    'size': 3},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 65581.34062252834,\n",
       "    'KL_theta_train': 108485.05644122555,\n",
       "    'KL_theta_test': 27005.42818337353,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 65581.34062252834,\n",
       "    'KL_theta_train': 108485.05644122555,\n",
       "    'KL_theta_test': 27005.42818337353,\n",
       "    'size': 0}}, {'FWD': {'nll': 99.09596913339705,\n",
       "    'KL': 53384.025682677486,\n",
       "    'KL_theta_train': 107630.37396349585,\n",
       "    'KL_theta_test': 27204.904647059266,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 53447.316883862135,\n",
       "    'KL_theta_train': 108422.4976979091,\n",
       "    'KL_theta_test': 27041.498105695162,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 99.9620661306309,\n",
       "    'KL': 53433.75882934575,\n",
       "    'KL_theta_train': 108445.05642278137,\n",
       "    'KL_theta_test': 27039.617215541777,\n",
       "    'size': 1}}, {'FWD': {'nll': 104.69952269944345,\n",
       "    'KL': 106221.69100182077,\n",
       "    'KL_theta_train': 110826.72967304109,\n",
       "    'KL_theta_test': 27200.185860713667,\n",
       "    'size': 3},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 104887.60664696463,\n",
       "    'KL_theta_train': 108331.10115308242,\n",
       "    'KL_theta_test': 27020.433594202044,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 104887.60664696463,\n",
       "    'KL_theta_train': 108331.10115308242,\n",
       "    'KL_theta_test': 27020.433594202044,\n",
       "    'size': 0}}, {'FWD': {'nll': 100.74920099926453,\n",
       "    'KL': 178463.69253343277,\n",
       "    'KL_theta_train': 108378.80097699733,\n",
       "    'KL_theta_test': 27282.797765018222,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.81905058787561,\n",
       "    'KL': 178755.06942663548,\n",
       "    'KL_theta_train': 108461.6565227978,\n",
       "    'KL_theta_test': 26868.490900131947,\n",
       "    'size': 1},\n",
       "   'SLOPE': {'nll': 100.27128259305715,\n",
       "    'KL': 177991.78944228537,\n",
       "    'KL_theta_train': 108704.74329806038,\n",
       "    'KL_theta_test': 26909.454138216515,\n",
       "    'size': 3}}, {'FWD': {'nll': 102.08575852461541,\n",
       "    'KL': 117325.94214716605,\n",
       "    'KL_theta_train': 108853.98867832974,\n",
       "    'KL_theta_test': 27144.904814649493,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.00490322623692,\n",
       "    'KL': 116201.33961866175,\n",
       "    'KL_theta_train': 108367.38299075127,\n",
       "    'KL_theta_test': 27028.05460170865,\n",
       "    'size': 1},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 116198.55369656436,\n",
       "    'KL_theta_train': 108383.6361784506,\n",
       "    'KL_theta_test': 27030.921405604673,\n",
       "    'size': 0}}, {'FWD': {'nll': 99.98892582312834,\n",
       "    'KL': 78726.89481461023,\n",
       "    'KL_theta_train': 108252.6359109227,\n",
       "    'KL_theta_test': 26912.362420620197,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.28387720737828,\n",
       "    'KL': 78906.38540214681,\n",
       "    'KL_theta_train': 107982.38347092536,\n",
       "    'KL_theta_test': 26935.225433804575,\n",
       "    'size': 6},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 78945.04120768177,\n",
       "    'KL_theta_train': 108390.35427528899,\n",
       "    'KL_theta_test': 27011.44097681997,\n",
       "    'size': 0}}, {'FWD': {'nll': 100.19691917376379,\n",
       "    'KL': 64271.98790333636,\n",
       "    'KL_theta_train': 108182.4883973066,\n",
       "    'KL_theta_test': 27035.72109299094,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 64109.918458841144,\n",
       "    'KL_theta_train': 108401.00137865894,\n",
       "    'KL_theta_test': 27006.911598836636,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.00177302948264,\n",
       "    'KL': 64266.64050764564,\n",
       "    'KL_theta_train': 108392.37002838396,\n",
       "    'KL_theta_test': 27122.78973939477,\n",
       "    'size': 4}}, {'FWD': {'nll': 100.87597601124315,\n",
       "    'KL': 65059.577854977,\n",
       "    'KL_theta_train': 108030.93722554702,\n",
       "    'KL_theta_test': 27029.23565218507,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 64799.37550329351,\n",
       "    'KL_theta_train': 108401.4690432477,\n",
       "    'KL_theta_test': 27010.36663449367,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 99.54341311216712,\n",
       "    'KL': 63495.31888917332,\n",
       "    'KL_theta_train': 109005.15335717787,\n",
       "    'KL_theta_test': 26398.891127676296,\n",
       "    'size': 1}}, {'FWD': {'nll': 101.56137007183965,\n",
       "    'KL': 107459.80208940063,\n",
       "    'KL_theta_train': 108008.71873937965,\n",
       "    'KL_theta_test': 27590.220278146462,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 102.31877475290554,\n",
       "    'KL': 108670.3692916777,\n",
       "    'KL_theta_train': 110008.91437509534,\n",
       "    'KL_theta_test': 26863.113716786742,\n",
       "    'size': 1},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 106629.15293835358,\n",
       "    'KL_theta_train': 108430.9917400384,\n",
       "    'KL_theta_test': 27001.937102723306,\n",
       "    'size': 0}}, {'FWD': {'nll': 98.01227067692079,\n",
       "    'KL': 152694.3517036962,\n",
       "    'KL_theta_train': 112638.05852662542,\n",
       "    'KL_theta_test': 26341.634442921997,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.73755776511686,\n",
       "    'KL': 158361.87865416057,\n",
       "    'KL_theta_train': 106596.48877970944,\n",
       "    'KL_theta_test': 26880.413001746063,\n",
       "    'size': 8},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 159373.41534238486,\n",
       "    'KL_theta_train': 108471.33205098078,\n",
       "    'KL_theta_test': 27017.06750562717,\n",
       "    'size': 0}}, {'FWD': {'nll': 102.24701704272199,\n",
       "    'KL': 101089.24296390594,\n",
       "    'KL_theta_train': 108748.36716124119,\n",
       "    'KL_theta_test': 27124.605298709277,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 101944.77654627457,\n",
       "    'KL_theta_train': 108377.65171929864,\n",
       "    'KL_theta_test': 27005.915077857248,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 101944.77654627457,\n",
       "    'KL_theta_train': 108377.65171929864,\n",
       "    'KL_theta_test': 27005.915077857248,\n",
       "    'size': 0}}, {'FWD': {'nll': 103.30573872853921,\n",
       "    'KL': 320608.69356971333,\n",
       "    'KL_theta_train': 109140.6260490317,\n",
       "    'KL_theta_test': 27359.498511708603,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 99.80800076181623,\n",
       "    'KL': 324723.76961067,\n",
       "    'KL_theta_train': 108206.9302767437,\n",
       "    'KL_theta_test': 27038.552930854843,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.44529796394859,\n",
       "    'KL': 322853.0204887153,\n",
       "    'KL_theta_train': 107957.95182328588,\n",
       "    'KL_theta_test': 26787.519653163217,\n",
       "    'size': 3}}, {'FWD': {'nll': 99.62689446682963,\n",
       "    'KL': 157785.01362930055,\n",
       "    'KL_theta_train': 108000.6315227738,\n",
       "    'KL_theta_test': 27021.887617416483,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 102.5773651984778,\n",
       "    'KL': 156468.7801488998,\n",
       "    'KL_theta_train': 107609.71223694333,\n",
       "    'KL_theta_test': 28022.331799014304,\n",
       "    'size': 11},\n",
       "   'SLOPE': {'nll': 100.42636730262741,\n",
       "    'KL': 157198.08652366567,\n",
       "    'KL_theta_train': 108592.87190577076,\n",
       "    'KL_theta_test': 27085.202137230604,\n",
       "    'size': 3}}, {'FWD': {'nll': 106.54104626612634,\n",
       "    'KL': 108024.6745869259,\n",
       "    'KL_theta_train': 106291.94656009645,\n",
       "    'KL_theta_test': 26059.64032231533,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 108571.3329771109,\n",
       "    'KL_theta_train': 108339.39281096628,\n",
       "    'KL_theta_test': 26990.026838095164,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.20306373540103,\n",
       "    'KL': 108273.18587098764,\n",
       "    'KL_theta_train': 108371.60923173267,\n",
       "    'KL_theta_test': 27122.28262698141,\n",
       "    'size': 3}}, {'FWD': {'nll': 102.1746248447009,\n",
       "    'KL': 133327.1749115094,\n",
       "    'KL_theta_train': 107297.35133425373,\n",
       "    'KL_theta_test': 27338.760064904684,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 132530.63526117348,\n",
       "    'KL_theta_train': 108323.04098775247,\n",
       "    'KL_theta_test': 26976.405255428144,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.19612229895904,\n",
       "    'KL': 133021.5825105458,\n",
       "    'KL_theta_train': 108421.45851501547,\n",
       "    'KL_theta_test': 27066.5504002074,\n",
       "    'size': 2}}, {'FWD': {'nll': 105.72693269298928,\n",
       "    'KL': 114258.579987335,\n",
       "    'KL_theta_train': 106742.8636893096,\n",
       "    'KL_theta_test': 26638.99937451711,\n",
       "    'size': 3},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 111933.52444792022,\n",
       "    'KL_theta_train': 108416.08050954639,\n",
       "    'KL_theta_test': 27034.94840046519,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 111933.52444792022,\n",
       "    'KL_theta_train': 108416.08050954639,\n",
       "    'KL_theta_test': 27034.94840046519,\n",
       "    'size': 0}}, {'FWD': {'nll': 100.37252529957358,\n",
       "    'KL': 55152.33622591496,\n",
       "    'KL_theta_train': 108559.62943193037,\n",
       "    'KL_theta_test': 27032.253766080394,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 99.93782515395993,\n",
       "    'KL': 53978.90029892486,\n",
       "    'KL_theta_train': 107535.51805354322,\n",
       "    'KL_theta_test': 26302.977707766928,\n",
       "    'size': 5},\n",
       "   'SLOPE': {'nll': 101.96286199661458,\n",
       "    'KL': 55580.77014047892,\n",
       "    'KL_theta_train': 109486.16581620442,\n",
       "    'KL_theta_test': 26853.138849582454,\n",
       "    'size': 1}}, {'FWD': {'nll': 101.53272460072803,\n",
       "    'KL': 95047.71995698934,\n",
       "    'KL_theta_train': 107680.92542265405,\n",
       "    'KL_theta_test': 27098.53474905162,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 98.35242597238545,\n",
       "    'KL': 94608.34637152316,\n",
       "    'KL_theta_train': 107269.84556186778,\n",
       "    'KL_theta_test': 27590.311717714285,\n",
       "    'size': 6},\n",
       "   'SLOPE': {'nll': 100.0376519411503,\n",
       "    'KL': 95185.06072901019,\n",
       "    'KL_theta_train': 108459.63045230255,\n",
       "    'KL_theta_test': 27039.011353662154,\n",
       "    'size': 2}}, {'FWD': {'nll': 102.42058175377441,\n",
       "    'KL': 125622.30791775236,\n",
       "    'KL_theta_train': 106843.73346334968,\n",
       "    'KL_theta_test': 27096.51285187014,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 99.99131073963697,\n",
       "    'KL': 123695.54457879264,\n",
       "    'KL_theta_train': 108444.75703249712,\n",
       "    'KL_theta_test': 27025.13107525083,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 123717.67719680062,\n",
       "    'KL_theta_train': 108414.74983267649,\n",
       "    'KL_theta_test': 27024.57391458606,\n",
       "    'size': 0}}, {'FWD': {'nll': 102.18296204736411,\n",
       "    'KL': 115413.26177767747,\n",
       "    'KL_theta_train': 107257.80563588298,\n",
       "    'KL_theta_test': 27623.34404648157,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 113525.81927020737,\n",
       "    'KL_theta_train': 108381.2795852235,\n",
       "    'KL_theta_test': 27009.788372803487,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 99.9077661788959,\n",
       "    'KL': 113569.01640693466,\n",
       "    'KL_theta_train': 108546.02157082502,\n",
       "    'KL_theta_test': 27043.43029979196,\n",
       "    'size': 1}}, {'FWD': {'nll': 101.28145031888276,\n",
       "    'KL': 122248.38625785323,\n",
       "    'KL_theta_train': 106998.86774622132,\n",
       "    'KL_theta_test': 27215.27358058481,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 122125.36814782913,\n",
       "    'KL_theta_train': 108363.8895463533,\n",
       "    'KL_theta_test': 27002.05248459231,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 99.51174755165836,\n",
       "    'KL': 121912.12642466046,\n",
       "    'KL_theta_train': 108413.8844094353,\n",
       "    'KL_theta_test': 27121.2124010488,\n",
       "    'size': 2}}, {'FWD': {'nll': 98.59646592157257,\n",
       "    'KL': 176547.0569072686,\n",
       "    'KL_theta_train': 108435.83545858903,\n",
       "    'KL_theta_test': 27484.82987918237,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 99.87665726200713,\n",
       "    'KL': 176614.1600838112,\n",
       "    'KL_theta_train': 108439.17170474963,\n",
       "    'KL_theta_test': 27025.86745284265,\n",
       "    'size': 1},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 176665.02117241087,\n",
       "    'KL_theta_train': 108420.35609202465,\n",
       "    'KL_theta_test': 27010.651314097366,\n",
       "    'size': 0}}, {'FWD': {'nll': 100.92835117346856,\n",
       "    'KL': 163409.37195644432,\n",
       "    'KL_theta_train': 108102.48652532064,\n",
       "    'KL_theta_test': 27155.586794233695,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.57145743148526,\n",
       "    'KL': 162065.3204065353,\n",
       "    'KL_theta_train': 108439.49848745632,\n",
       "    'KL_theta_test': 26990.877696905733,\n",
       "    'size': 1},\n",
       "   'SLOPE': {'nll': 100.29973623596302,\n",
       "    'KL': 162552.98400227836,\n",
       "    'KL_theta_train': 108646.73275186768,\n",
       "    'KL_theta_test': 27026.274741469555,\n",
       "    'size': 2}}, {'FWD': {'nll': 100.08189344230743,\n",
       "    'KL': 154244.7451420831,\n",
       "    'KL_theta_train': 108466.40960794296,\n",
       "    'KL_theta_test': 26950.88616066337,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 154879.3735358706,\n",
       "    'KL_theta_train': 108430.33585461118,\n",
       "    'KL_theta_test': 27019.391338619123,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.02972080040692,\n",
       "    'KL': 154890.97558278864,\n",
       "    'KL_theta_train': 108254.24492036569,\n",
       "    'KL_theta_test': 26992.80019590025,\n",
       "    'size': 0}}, {'FWD': {'nll': 99.92869051787928,\n",
       "    'KL': 81703.80743832895,\n",
       "    'KL_theta_train': 108406.33201464143,\n",
       "    'KL_theta_test': 26990.396991912727,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 99.87384017910487,\n",
       "    'KL': 81883.83365034052,\n",
       "    'KL_theta_train': 107610.57970204049,\n",
       "    'KL_theta_test': 26997.65259971925,\n",
       "    'size': 2},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 82008.74396697182,\n",
       "    'KL_theta_train': 108289.51599893637,\n",
       "    'KL_theta_test': 27023.28858475841,\n",
       "    'size': 0}}, {'FWD': {'nll': 100.76513225141564,\n",
       "    'KL': 117461.98217454435,\n",
       "    'KL_theta_train': 108215.27267688647,\n",
       "    'KL_theta_test': 27336.821273978116,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 102.16544698806996,\n",
       "    'KL': 119839.96453508956,\n",
       "    'KL_theta_train': 108517.26465972507,\n",
       "    'KL_theta_test': 27127.526015695887,\n",
       "    'size': 3},\n",
       "   'SLOPE': {'nll': 103.45256409874152,\n",
       "    'KL': 117284.65489263728,\n",
       "    'KL_theta_train': 109199.41774383624,\n",
       "    'KL_theta_test': 27187.334467289536,\n",
       "    'size': 5}}, {'FWD': {'nll': 101.31212451782868,\n",
       "    'KL': 87898.2319390157,\n",
       "    'KL_theta_train': 109466.73329686432,\n",
       "    'KL_theta_test': 27155.22148842009,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 87757.98504537033,\n",
       "    'KL_theta_train': 108406.68104987557,\n",
       "    'KL_theta_test': 26997.800488798686,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 99.84321514867986,\n",
       "    'KL': 87821.0996831026,\n",
       "    'KL_theta_train': 109792.10678550471,\n",
       "    'KL_theta_test': 27244.802926522963,\n",
       "    'size': 2}}, {'FWD': {'nll': 99.9520028472923,\n",
       "    'KL': 2578836.0847782698,\n",
       "    'KL_theta_train': 107869.04982433138,\n",
       "    'KL_theta_test': 27039.59476086567,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 2577564.3944823034,\n",
       "    'KL_theta_train': 108300.11752440932,\n",
       "    'KL_theta_test': 27002.8140464335,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 2577564.3944823034,\n",
       "    'KL_theta_train': 108300.11752440932,\n",
       "    'KL_theta_test': 27002.8140464335,\n",
       "    'size': 0}}, {'FWD': {'nll': 101.48135377964346,\n",
       "    'KL': 225414.99390712092,\n",
       "    'KL_theta_train': 109834.43898688993,\n",
       "    'KL_theta_test': 26842.37280393631,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.70724727802082,\n",
       "    'KL': 226572.60898094368,\n",
       "    'KL_theta_train': 108543.37760065442,\n",
       "    'KL_theta_test': 27273.291886000185,\n",
       "    'size': 5},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 224771.5125018902,\n",
       "    'KL_theta_train': 108443.71809764387,\n",
       "    'KL_theta_test': 27030.65585198748,\n",
       "    'size': 0}}, {'FWD': {'nll': 101.16737612194609,\n",
       "    'KL': 256444.28085256144,\n",
       "    'KL_theta_train': 110395.19605442391,\n",
       "    'KL_theta_test': 27786.145600762433,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 255138.4540556581,\n",
       "    'KL_theta_train': 108439.4841341105,\n",
       "    'KL_theta_test': 26976.37812863492,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 255138.4540556581,\n",
       "    'KL_theta_train': 108439.4841341105,\n",
       "    'KL_theta_test': 26976.37812863492,\n",
       "    'size': 0}}, {'FWD': {'nll': 101.79112367115859,\n",
       "    'KL': 231120.76355306106,\n",
       "    'KL_theta_train': 107780.96238081437,\n",
       "    'KL_theta_test': 26958.03850388162,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 229576.77577925357,\n",
       "    'KL_theta_train': 108419.96897656348,\n",
       "    'KL_theta_test': 27010.1076391755,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 229576.77577925357,\n",
       "    'KL_theta_train': 108419.96897656348,\n",
       "    'KL_theta_test': 27010.1076391755,\n",
       "    'size': 0}}, {'FWD': {'nll': 103.64297933155751,\n",
       "    'KL': 103086.57910565495,\n",
       "    'KL_theta_train': 106999.16998336959,\n",
       "    'KL_theta_test': 26956.992562963005,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 100851.57572950631,\n",
       "    'KL_theta_train': 108371.87557163273,\n",
       "    'KL_theta_test': 27033.371318382004,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 100851.57572950631,\n",
       "    'KL_theta_train': 108371.87557163273,\n",
       "    'KL_theta_test': 27033.371318382004,\n",
       "    'size': 0}}, {'FWD': {'nll': 103.01460613449318,\n",
       "    'KL': 266147.2609531535,\n",
       "    'KL_theta_train': 107912.45379271754,\n",
       "    'KL_theta_test': 26800.22668440224,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 266112.30555485486,\n",
       "    'KL_theta_train': 108339.49825531384,\n",
       "    'KL_theta_test': 26988.01641155855,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 266112.30555485486,\n",
       "    'KL_theta_train': 108339.49825531384,\n",
       "    'KL_theta_test': 26988.01641155855,\n",
       "    'size': 0}}, {'FWD': {'nll': 104.04854334946631,\n",
       "    'KL': 302669.24859455595,\n",
       "    'KL_theta_train': 108674.25223081459,\n",
       "    'KL_theta_test': 27098.16846614727,\n",
       "    'size': 3},\n",
       "   'LASSO': {'nll': 101.25729774482883,\n",
       "    'KL': 297033.9856587635,\n",
       "    'KL_theta_train': 107948.30388600643,\n",
       "    'KL_theta_test': 27115.597772872494,\n",
       "    'size': 3},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 296927.16260093957,\n",
       "    'KL_theta_train': 108526.84456544605,\n",
       "    'KL_theta_test': 27007.434011691832,\n",
       "    'size': 0}}, {'FWD': {'nll': 100.6575636236152,\n",
       "    'KL': 139833.90689496289,\n",
       "    'KL_theta_train': 108237.66254660269,\n",
       "    'KL_theta_test': 26877.056861506204,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.09577453685962,\n",
       "    'KL': 140166.29707898837,\n",
       "    'KL_theta_train': 108343.205235398,\n",
       "    'KL_theta_test': 26976.558314944978,\n",
       "    'size': 1},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 140101.14103967627,\n",
       "    'KL_theta_train': 108403.80398209624,\n",
       "    'KL_theta_test': 26993.18945754474,\n",
       "    'size': 0}}, {'FWD': {'nll': 99.75175674861839,\n",
       "    'KL': 141821.03646025673,\n",
       "    'KL_theta_train': 107961.61872584863,\n",
       "    'KL_theta_test': 27250.503356611778,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 101.51382824094657,\n",
       "    'KL': 143295.5002938473,\n",
       "    'KL_theta_train': 108366.45675361124,\n",
       "    'KL_theta_test': 27343.30128909301,\n",
       "    'size': 7},\n",
       "   'SLOPE': {'nll': 100.93275182043942,\n",
       "    'KL': 143748.44908270135,\n",
       "    'KL_theta_train': 109412.78351566641,\n",
       "    'KL_theta_test': 27400.940804674738,\n",
       "    'size': 5}}, {'FWD': {'nll': 99.91315669408235,\n",
       "    'KL': 82759.8523233737,\n",
       "    'KL_theta_train': 108431.27385555544,\n",
       "    'KL_theta_test': 27037.82067024397,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 82831.84910940696,\n",
       "    'KL_theta_train': 108391.1128621237,\n",
       "    'KL_theta_test': 26995.95223467259,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 82831.84910940696,\n",
       "    'KL_theta_train': 108391.1128621237,\n",
       "    'KL_theta_test': 26995.95223467259,\n",
       "    'size': 0}}, {'FWD': {'nll': 99.44557500023589,\n",
       "    'KL': 125755.33265270395,\n",
       "    'KL_theta_train': 108608.06415929252,\n",
       "    'KL_theta_test': 27235.16342852469,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 106.91521402730277,\n",
       "    'KL': 125583.31018964191,\n",
       "    'KL_theta_train': 112924.86087202467,\n",
       "    'KL_theta_test': 25791.950896964376,\n",
       "    'size': 11},\n",
       "   'SLOPE': {'nll': 100.07458435601131,\n",
       "    'KL': 125819.4771456819,\n",
       "    'KL_theta_train': 108384.59326110031,\n",
       "    'KL_theta_test': 27026.29590929158,\n",
       "    'size': 1}}, {'FWD': {'nll': 100.52421306955705,\n",
       "    'KL': 83689.8871537002,\n",
       "    'KL_theta_train': 108056.51245484351,\n",
       "    'KL_theta_test': 26728.237416917967,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.4455984873531,\n",
       "    'KL': 85075.35353903058,\n",
       "    'KL_theta_train': 108411.36677844528,\n",
       "    'KL_theta_test': 27026.94049773966,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.38814815648438,\n",
       "    'KL': 85049.77785325059,\n",
       "    'KL_theta_train': 108415.93340834325,\n",
       "    'KL_theta_test': 27022.401180129145,\n",
       "    'size': 0}}, {'FWD': {'nll': 100.55213276110514,\n",
       "    'KL': 94731.7694143046,\n",
       "    'KL_theta_train': 107005.27630754324,\n",
       "    'KL_theta_test': 27160.83019384151,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 99.91770063169638,\n",
       "    'KL': 94129.11871388828,\n",
       "    'KL_theta_train': 108862.04904092391,\n",
       "    'KL_theta_test': 26867.915900962103,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 94454.0034031714,\n",
       "    'KL_theta_train': 108425.92314375905,\n",
       "    'KL_theta_test': 26997.94139606935,\n",
       "    'size': 0}}, {'FWD': {'nll': 99.99137154522536,\n",
       "    'KL': 167821.38028554746,\n",
       "    'KL_theta_train': 108354.53241541065,\n",
       "    'KL_theta_test': 27052.871109892516,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 168322.83520979236,\n",
       "    'KL_theta_train': 108465.73439635735,\n",
       "    'KL_theta_test': 27032.064567856414,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 168322.83520979236,\n",
       "    'KL_theta_train': 108465.73439635735,\n",
       "    'KL_theta_test': 27032.064567856414,\n",
       "    'size': 0}}, {'FWD': {'nll': 99.91031246164906,\n",
       "    'KL': 226542.71421789838,\n",
       "    'KL_theta_train': 107469.5271951741,\n",
       "    'KL_theta_test': 26089.04350386652,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 102.06945827952472,\n",
       "    'KL': 233804.72987466486,\n",
       "    'KL_theta_train': 109005.96579561965,\n",
       "    'KL_theta_test': 27378.741590840156,\n",
       "    'size': 3},\n",
       "   'SLOPE': {'nll': 99.9498708344044,\n",
       "    'KL': 229821.5479451459,\n",
       "    'KL_theta_train': 108409.14476661588,\n",
       "    'KL_theta_test': 26896.170016044467,\n",
       "    'size': 0}}, {'FWD': {'nll': 99.0829544388176,\n",
       "    'KL': 216605.37325217648,\n",
       "    'KL_theta_train': 107510.9751651333,\n",
       "    'KL_theta_test': 27166.204292162103,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 217499.61383986496,\n",
       "    'KL_theta_train': 108412.2753732301,\n",
       "    'KL_theta_test': 26978.29127100817,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 97.96049232365687,\n",
       "    'KL': 214992.9195713546,\n",
       "    'KL_theta_train': 108654.60499942573,\n",
       "    'KL_theta_test': 27404.770102328264,\n",
       "    'size': 8}}, {'FWD': {'nll': 99.97791805083605,\n",
       "    'KL': 101628.92837459577,\n",
       "    'KL_theta_train': 108252.2112854041,\n",
       "    'KL_theta_test': 26995.859265146035,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.54600496887362,\n",
       "    'KL': 101429.91477324931,\n",
       "    'KL_theta_train': 108952.95006985796,\n",
       "    'KL_theta_test': 26737.811326549716,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.2000386401719,\n",
       "    'KL': 101535.72467579256,\n",
       "    'KL_theta_train': 108608.75191525545,\n",
       "    'KL_theta_test': 26886.25314787155,\n",
       "    'size': 0}}, {'FWD': {'nll': 99.6159608166451,\n",
       "    'KL': 175899.18201702874,\n",
       "    'KL_theta_train': 109225.80972240849,\n",
       "    'KL_theta_test': 27006.657968529253,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 101.377762034214,\n",
       "    'KL': 177189.7379928304,\n",
       "    'KL_theta_train': 110354.14964262448,\n",
       "    'KL_theta_test': 26698.949678303234,\n",
       "    'size': 2},\n",
       "   'SLOPE': {'nll': 101.10781861673189,\n",
       "    'KL': 179835.6381467544,\n",
       "    'KL_theta_train': 108911.3558437647,\n",
       "    'KL_theta_test': 27180.616792704877,\n",
       "    'size': 2}}, {'FWD': {'nll': 102.24252938625736,\n",
       "    'KL': 63623.832553169086,\n",
       "    'KL_theta_train': 109000.08513614617,\n",
       "    'KL_theta_test': 26935.883695065633,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.97957444100055,\n",
       "    'KL': 62585.46094251989,\n",
       "    'KL_theta_train': 109567.42510100719,\n",
       "    'KL_theta_test': 27038.970081838244,\n",
       "    'size': 8},\n",
       "   'SLOPE': {'nll': 100.01329465410222,\n",
       "    'KL': 62946.57165548147,\n",
       "    'KL_theta_train': 108416.8374062435,\n",
       "    'KL_theta_test': 26999.81947959616,\n",
       "    'size': 1}}, {'FWD': {'nll': 99.65782969261181,\n",
       "    'KL': 166433.04093029822,\n",
       "    'KL_theta_train': 107960.487783716,\n",
       "    'KL_theta_test': 27026.110682769915,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 102.33407906663871,\n",
       "    'KL': 168909.6462443558,\n",
       "    'KL_theta_train': 108038.76684328145,\n",
       "    'KL_theta_test': 26656.24651700714,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 99.88103455353128,\n",
       "    'KL': 167783.53887058859,\n",
       "    'KL_theta_train': 110091.0264265553,\n",
       "    'KL_theta_test': 28319.433353983764,\n",
       "    'size': 2}}, {'FWD': {'nll': 100.96328505211038,\n",
       "    'KL': 58214.48036024837,\n",
       "    'KL_theta_train': 109170.74089248173,\n",
       "    'KL_theta_test': 27094.427447819537,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 57712.34613250626,\n",
       "    'KL_theta_train': 108389.5539610209,\n",
       "    'KL_theta_test': 26998.810187660583,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 57712.34613250626,\n",
       "    'KL_theta_train': 108389.5539610209,\n",
       "    'KL_theta_test': 26998.810187660583,\n",
       "    'size': 0}}, {'FWD': {'nll': 101.97842743085855,\n",
       "    'KL': 69725.19382694796,\n",
       "    'KL_theta_train': 107488.30244557273,\n",
       "    'KL_theta_test': 26836.240532349595,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.10055052213734,\n",
       "    'KL': 69618.69647813059,\n",
       "    'KL_theta_train': 108475.92453634558,\n",
       "    'KL_theta_test': 26976.779694291057,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 104.52872950825252,\n",
       "    'KL': 70440.08735832802,\n",
       "    'KL_theta_train': 108199.10663970913,\n",
       "    'KL_theta_test': 26709.03102157882,\n",
       "    'size': 3}}, {'FWD': {'nll': 100.80488869698394,\n",
       "    'KL': 118279.05965073367,\n",
       "    'KL_theta_train': 107581.30742315945,\n",
       "    'KL_theta_test': 27076.694584010365,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 118222.97110166926,\n",
       "    'KL_theta_train': 108425.21116705338,\n",
       "    'KL_theta_test': 26976.819544112812,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.32316124861602,\n",
       "    'KL': 118140.2726752324,\n",
       "    'KL_theta_train': 108423.94007812411,\n",
       "    'KL_theta_test': 26990.535642680512,\n",
       "    'size': 2}}, {'FWD': {'nll': 99.23031960993936,\n",
       "    'KL': 129026.77909351347,\n",
       "    'KL_theta_train': 110845.34615596739,\n",
       "    'KL_theta_test': 26360.01958010405,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.2698112488514,\n",
       "    'KL': 134619.41844571746,\n",
       "    'KL_theta_train': 108911.65692033873,\n",
       "    'KL_theta_test': 27083.53730534299,\n",
       "    'size': 1},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 135468.00006753672,\n",
       "    'KL_theta_train': 108354.2834722554,\n",
       "    'KL_theta_test': 26984.42501184339,\n",
       "    'size': 0}}, {'FWD': {'nll': 101.77316695435061,\n",
       "    'KL': 80837.977212198,\n",
       "    'KL_theta_train': 106249.01408756824,\n",
       "    'KL_theta_test': 27127.744570775212,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 78957.80242038025,\n",
       "    'KL_theta_train': 108389.59659851866,\n",
       "    'KL_theta_test': 27026.563939760257,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 78957.80242038025,\n",
       "    'KL_theta_train': 108389.59659851866,\n",
       "    'KL_theta_test': 27026.563939760257,\n",
       "    'size': 0}}, {'FWD': {'nll': 98.6076993262207,\n",
       "    'KL': 147526.93740032404,\n",
       "    'KL_theta_train': 107217.5429078649,\n",
       "    'KL_theta_test': 26460.91099959518,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 150061.33481912443,\n",
       "    'KL_theta_train': 108415.05997528693,\n",
       "    'KL_theta_test': 27031.721099876755,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.09811695273832,\n",
       "    'KL': 150081.54861029793,\n",
       "    'KL_theta_train': 108508.84385037681,\n",
       "    'KL_theta_test': 27008.31096260725,\n",
       "    'size': 1}}, {'FWD': {'nll': 107.13421106558327,\n",
       "    'KL': 404126.80852666247,\n",
       "    'KL_theta_train': 112561.50224951206,\n",
       "    'KL_theta_test': 28123.62113066906,\n",
       "    'size': 3},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 376317.9971876439,\n",
       "    'KL_theta_train': 108409.26824540342,\n",
       "    'KL_theta_test': 27011.90962002454,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 99.90867348853777,\n",
       "    'KL': 357095.070495622,\n",
       "    'KL_theta_train': 108416.32106126292,\n",
       "    'KL_theta_test': 26888.132131267415,\n",
       "    'size': 8}}, {'FWD': {'nll': 99.62573502661776,\n",
       "    'KL': 75227.86088474763,\n",
       "    'KL_theta_train': 108471.50082123216,\n",
       "    'KL_theta_test': 27706.443937449752,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.08608306735574,\n",
       "    'KL': 74757.69681439454,\n",
       "    'KL_theta_train': 108139.58325308171,\n",
       "    'KL_theta_test': 27065.35326473813,\n",
       "    'size': 1},\n",
       "   'SLOPE': {'nll': 100.16883377245244,\n",
       "    'KL': 74819.06661927995,\n",
       "    'KL_theta_train': 108633.00818427763,\n",
       "    'KL_theta_test': 27026.828243485113,\n",
       "    'size': 1}}, {'FWD': {'nll': 102.00299792353071,\n",
       "    'KL': 103020.95354724671,\n",
       "    'KL_theta_train': 108402.51444388606,\n",
       "    'KL_theta_test': 26187.573513360025,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 99.8736621918799,\n",
       "    'KL': 103745.29367476534,\n",
       "    'KL_theta_train': 108390.20995693459,\n",
       "    'KL_theta_test': 27014.379780722244,\n",
       "    'size': 4},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 103886.9423583475,\n",
       "    'KL_theta_train': 108354.82795079781,\n",
       "    'KL_theta_test': 27052.531985530906,\n",
       "    'size': 0}}, {'FWD': {'nll': 99.8368558699375,\n",
       "    'KL': 106796.62147756973,\n",
       "    'KL_theta_train': 108705.79518265603,\n",
       "    'KL_theta_test': 26816.60506675664,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 107411.1465109571,\n",
       "    'KL_theta_train': 108399.24423864987,\n",
       "    'KL_theta_test': 27024.587569336665,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 107411.1465109571,\n",
       "    'KL_theta_train': 108399.24423864987,\n",
       "    'KL_theta_test': 27024.587569336665,\n",
       "    'size': 0}}, {'FWD': {'nll': 100.46636008446796,\n",
       "    'KL': 162028.9489850814,\n",
       "    'KL_theta_train': 106032.28707936991,\n",
       "    'KL_theta_test': 27212.155982384615,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 162483.73764588634,\n",
       "    'KL_theta_train': 108313.66636469074,\n",
       "    'KL_theta_test': 27023.360564997653,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 162483.73764588634,\n",
       "    'KL_theta_train': 108313.66636469074,\n",
       "    'KL_theta_test': 27023.360564997653,\n",
       "    'size': 0}}, {'FWD': {'nll': 99.9398261551983,\n",
       "    'KL': 40198.124342448325,\n",
       "    'KL_theta_train': 106887.80473616549,\n",
       "    'KL_theta_test': 27131.597282482777,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 40204.48895730864,\n",
       "    'KL_theta_train': 108309.76089561565,\n",
       "    'KL_theta_test': 27027.174337409826,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 40204.48895730864,\n",
       "    'KL_theta_train': 108309.76089561565,\n",
       "    'KL_theta_test': 27027.174337409826,\n",
       "    'size': 0}}, {'FWD': {'nll': 100.036901374908,\n",
       "    'KL': 83943.14871880283,\n",
       "    'KL_theta_train': 108355.30796654547,\n",
       "    'KL_theta_test': 26980.477324348652,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.4670253559804,\n",
       "    'KL': 83540.35069870805,\n",
       "    'KL_theta_train': 108602.3006473139,\n",
       "    'KL_theta_test': 26717.16684826296,\n",
       "    'size': 1},\n",
       "   'SLOPE': {'nll': 100.11323581553756,\n",
       "    'KL': 83820.7402359191,\n",
       "    'KL_theta_train': 108483.09885421064,\n",
       "    'KL_theta_test': 26907.351207414322,\n",
       "    'size': 1}}, {'FWD': {'nll': 100.35339875782692,\n",
       "    'KL': 58322.75712737738,\n",
       "    'KL_theta_train': 108365.44767800573,\n",
       "    'KL_theta_test': 27119.479574942612,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 99.47281282244394,\n",
       "    'KL': 57740.380775053956,\n",
       "    'KL_theta_train': 108060.78804123306,\n",
       "    'KL_theta_test': 26944.161237946708,\n",
       "    'size': 2},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 58103.32869212369,\n",
       "    'KL_theta_train': 108355.45521196615,\n",
       "    'KL_theta_test': 27018.619126796533,\n",
       "    'size': 0}}, {'FWD': {'nll': 101.86626432681678,\n",
       "    'KL': 117440.34597601468,\n",
       "    'KL_theta_train': 108830.10609271516,\n",
       "    'KL_theta_test': 26985.553944957668,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.64884961917252,\n",
       "    'KL': 115655.65622382342,\n",
       "    'KL_theta_train': 108646.0225718258,\n",
       "    'KL_theta_test': 26873.074345317666,\n",
       "    'size': 2},\n",
       "   'SLOPE': {'nll': 100.5131689888262,\n",
       "    'KL': 116530.90732640163,\n",
       "    'KL_theta_train': 108855.24960766557,\n",
       "    'KL_theta_test': 26977.677727491653,\n",
       "    'size': 2}}, {'FWD': {'nll': 100.04748963002307,\n",
       "    'KL': 118926.18499178148,\n",
       "    'KL_theta_train': 108396.71834486957,\n",
       "    'KL_theta_test': 26886.249773278952,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 118763.1766770198,\n",
       "    'KL_theta_train': 108356.8205424604,\n",
       "    'KL_theta_test': 27010.112550107722,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.01648470074359,\n",
       "    'KL': 118730.1866933084,\n",
       "    'KL_theta_train': 108514.71964411666,\n",
       "    'KL_theta_test': 27042.213453071035,\n",
       "    'size': 0}}, {'FWD': {'nll': 101.36608157434861,\n",
       "    'KL': 206533.9653943442,\n",
       "    'KL_theta_train': 109262.39953801656,\n",
       "    'KL_theta_test': 27174.28452552137,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 105.49908925141231,\n",
       "    'KL': 207371.50877263313,\n",
       "    'KL_theta_train': 109835.66663856638,\n",
       "    'KL_theta_test': 27977.936326313236,\n",
       "    'size': 8},\n",
       "   'SLOPE': {'nll': 99.86042578723615,\n",
       "    'KL': 203437.07465651497,\n",
       "    'KL_theta_train': 108318.41698463573,\n",
       "    'KL_theta_test': 27044.86447712633,\n",
       "    'size': 2}}, {'FWD': {'nll': 100.24979556102355,\n",
       "    'KL': 305493.1285208189,\n",
       "    'KL_theta_train': 109228.46185023469,\n",
       "    'KL_theta_test': 26832.39351638277,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 306182.31019150076,\n",
       "    'KL_theta_train': 108429.72436388102,\n",
       "    'KL_theta_test': 27025.9762633525,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 306182.31019150076,\n",
       "    'KL_theta_train': 108429.72436388102,\n",
       "    'KL_theta_test': 27025.9762633525,\n",
       "    'size': 0}}, {'FWD': {'nll': 102.03905927552854,\n",
       "    'KL': 89252.21281789897,\n",
       "    'KL_theta_train': 109241.11774714859,\n",
       "    'KL_theta_test': 26630.290150670415,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 99.20358533517565,\n",
       "    'KL': 89317.81086072345,\n",
       "    'KL_theta_train': 108047.8287543367,\n",
       "    'KL_theta_test': 27334.519329319683,\n",
       "    'size': 2},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 89337.53288164332,\n",
       "    'KL_theta_train': 108342.22258693218,\n",
       "    'KL_theta_test': 27032.995906056145,\n",
       "    'size': 0}}, {'FWD': {'nll': 103.79220803981298,\n",
       "    'KL': 228529.6803675317,\n",
       "    'KL_theta_train': 109808.56705961641,\n",
       "    'KL_theta_test': 26531.418042434154,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.25656886599633,\n",
       "    'KL': 224783.625680035,\n",
       "    'KL_theta_train': 108033.3185585517,\n",
       "    'KL_theta_test': 26891.7059608772,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.68098483909235,\n",
       "    'KL': 224197.62367371182,\n",
       "    'KL_theta_train': 107649.20736111682,\n",
       "    'KL_theta_test': 26739.94441729929,\n",
       "    'size': 0}}, {'FWD': {'nll': 98.53335406349508,\n",
       "    'KL': 67644.75487328583,\n",
       "    'KL_theta_train': 109029.20541431199,\n",
       "    'KL_theta_test': 27116.019241669434,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.1143999525986,\n",
       "    'KL': 69515.65026105868,\n",
       "    'KL_theta_train': 108492.57742400374,\n",
       "    'KL_theta_test': 26943.406203402978,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 69576.27419683935,\n",
       "    'KL_theta_train': 108374.38404895915,\n",
       "    'KL_theta_test': 26984.135062481117,\n",
       "    'size': 0}}, {'FWD': {'nll': 100.09869609186099,\n",
       "    'KL': 157957.9910694523,\n",
       "    'KL_theta_train': 108485.39323512968,\n",
       "    'KL_theta_test': 26796.490149631583,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 99.99881139106955,\n",
       "    'KL': 157805.74785865078,\n",
       "    'KL_theta_train': 108410.85065041462,\n",
       "    'KL_theta_test': 27005.94899422011,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 99.97270154002827,\n",
       "    'KL': 157720.25438382817,\n",
       "    'KL_theta_train': 108507.23302264969,\n",
       "    'KL_theta_test': 27003.968485597416,\n",
       "    'size': 0}}, {'FWD': {'nll': 100.37209078118471,\n",
       "    'KL': 101325.17845047139,\n",
       "    'KL_theta_train': 107559.66829119186,\n",
       "    'KL_theta_test': 27128.95715166034,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.86644510575024,\n",
       "    'KL': 102359.05841210761,\n",
       "    'KL_theta_train': 108458.37986326107,\n",
       "    'KL_theta_test': 26712.484891910797,\n",
       "    'size': 1},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 102024.53763504289,\n",
       "    'KL_theta_train': 108344.22852528217,\n",
       "    'KL_theta_test': 27005.606898843853,\n",
       "    'size': 0}}, {'FWD': {'nll': 100.74007063816428,\n",
       "    'KL': 150201.12704943612,\n",
       "    'KL_theta_train': 108372.80255116422,\n",
       "    'KL_theta_test': 26991.10383494194,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 101.12772413171676,\n",
       "    'KL': 151358.7043932986,\n",
       "    'KL_theta_train': 107325.25783341777,\n",
       "    'KL_theta_test': 27442.626123228987,\n",
       "    'size': 3},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 150038.49243637113,\n",
       "    'KL_theta_train': 108379.31974696525,\n",
       "    'KL_theta_test': 27030.03858338335,\n",
       "    'size': 0}}, {'FWD': {'nll': 100.78012045449488,\n",
       "    'KL': 199875.04938284997,\n",
       "    'KL_theta_train': 108269.2995825652,\n",
       "    'KL_theta_test': 26844.494957769264,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 199099.01428357174,\n",
       "    'KL_theta_train': 108411.61777729742,\n",
       "    'KL_theta_test': 27009.9535916172,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 199099.01428357174,\n",
       "    'KL_theta_train': 108411.61777729742,\n",
       "    'KL_theta_test': 27009.9535916172,\n",
       "    'size': 0}}, {'FWD': {'nll': 100.93823716827245,\n",
       "    'KL': 97767.89633703383,\n",
       "    'KL_theta_train': 107864.43854074334,\n",
       "    'KL_theta_test': 26866.943342717852,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 97247.62471481269,\n",
       "    'KL_theta_train': 108298.27015282403,\n",
       "    'KL_theta_test': 27001.391397377287,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 97247.62471481269,\n",
       "    'KL_theta_train': 108298.27015282403,\n",
       "    'KL_theta_test': 27001.391397377287,\n",
       "    'size': 0}}, {'FWD': {'nll': 100.85308853803315,\n",
       "    'KL': 180127.05406474686,\n",
       "    'KL_theta_train': 107855.27793327399,\n",
       "    'KL_theta_test': 26472.839975152,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.01510242372338,\n",
       "    'KL': 179093.93389600667,\n",
       "    'KL_theta_train': 108206.56510745065,\n",
       "    'KL_theta_test': 26932.239569427515,\n",
       "    'size': 2},\n",
       "   'SLOPE': {'nll': 99.44482152219499,\n",
       "    'KL': 178622.20697914326,\n",
       "    'KL_theta_train': 108304.04322427162,\n",
       "    'KL_theta_test': 26938.587031493076,\n",
       "    'size': 1}}, {'FWD': {'nll': 103.03093437551574,\n",
       "    'KL': 155869.78789931946,\n",
       "    'KL_theta_train': 108444.05615001183,\n",
       "    'KL_theta_test': 27139.586150603165,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 155666.83240280094,\n",
       "    'KL_theta_train': 108389.73850671455,\n",
       "    'KL_theta_test': 27016.073732249894,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.03899936881668,\n",
       "    'KL': 155726.52993451257,\n",
       "    'KL_theta_train': 108437.64485441301,\n",
       "    'KL_theta_test': 26998.469137230986,\n",
       "    'size': 0}}, {'FWD': {'nll': 100.47460268682052,\n",
       "    'KL': 188333.85529021127,\n",
       "    'KL_theta_train': 108614.54655730716,\n",
       "    'KL_theta_test': 27119.42804111017,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 99.88370560269117,\n",
       "    'KL': 186847.56026987292,\n",
       "    'KL_theta_train': 108419.65599194838,\n",
       "    'KL_theta_test': 26942.541086343143,\n",
       "    'size': 3},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 187667.22302874073,\n",
       "    'KL_theta_train': 108344.83177719393,\n",
       "    'KL_theta_test': 27024.56173924789,\n",
       "    'size': 0}}, {'FWD': {'nll': 99.56716292918847,\n",
       "    'KL': 436764.99045337347,\n",
       "    'KL_theta_train': 109042.02375489083,\n",
       "    'KL_theta_test': 26874.77324428706,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 440645.7643665029,\n",
       "    'KL_theta_train': 108399.63400575618,\n",
       "    'KL_theta_test': 26976.960672193996,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 440645.7643665029,\n",
       "    'KL_theta_train': 108399.63400575618,\n",
       "    'KL_theta_test': 26976.960672193996,\n",
       "    'size': 0}}, {'FWD': {'nll': 100.10488500692955,\n",
       "    'KL': 104062.72082369239,\n",
       "    'KL_theta_train': 108638.19241732717,\n",
       "    'KL_theta_test': 27053.259579797486,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 101.19852294339807,\n",
       "    'KL': 104873.85814640064,\n",
       "    'KL_theta_train': 110030.56383676469,\n",
       "    'KL_theta_test': 27157.339861016284,\n",
       "    'size': 4},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 103915.27848879635,\n",
       "    'KL_theta_train': 108518.4742982981,\n",
       "    'KL_theta_test': 27013.349305579482,\n",
       "    'size': 0}}, {'FWD': {'nll': 99.2321673145815,\n",
       "    'KL': 111333.66376920044,\n",
       "    'KL_theta_train': 109349.91729956411,\n",
       "    'KL_theta_test': 27080.313124309094,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 106.12208053920834,\n",
       "    'KL': 113410.56799992853,\n",
       "    'KL_theta_train': 106851.43719131658,\n",
       "    'KL_theta_test': 26252.376924540516,\n",
       "    'size': 4},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 111755.95107505395,\n",
       "    'KL_theta_train': 108457.68364544782,\n",
       "    'KL_theta_test': 27016.7404930586,\n",
       "    'size': 0}}, {'FWD': {'nll': 101.05845454481755,\n",
       "    'KL': 344920.9707297285,\n",
       "    'KL_theta_train': 109388.61829195794,\n",
       "    'KL_theta_test': 26476.148218621212,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 345907.9518127407,\n",
       "    'KL_theta_train': 108430.0269936035,\n",
       "    'KL_theta_test': 27037.008376575362,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.44941025309258,\n",
       "    'KL': 345666.0937445157,\n",
       "    'KL_theta_train': 108480.92982054861,\n",
       "    'KL_theta_test': 27123.61564907367,\n",
       "    'size': 1}}, {'FWD': {'nll': 106.86840215636285,\n",
       "    'KL': 122772.04508293683,\n",
       "    'KL_theta_train': 108694.70251241403,\n",
       "    'KL_theta_test': 26584.483947029712,\n",
       "    'size': 3},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 121839.65487569266,\n",
       "    'KL_theta_train': 108449.91343562753,\n",
       "    'KL_theta_test': 27006.861362262658,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.90275186495707,\n",
       "    'KL': 120439.42282078687,\n",
       "    'KL_theta_train': 108766.24052651068,\n",
       "    'KL_theta_test': 26699.7363556981,\n",
       "    'size': 5}}, {'FWD': {'nll': 101.75215076606605,\n",
       "    'KL': 109718.74436339704,\n",
       "    'KL_theta_train': 107399.04046781138,\n",
       "    'KL_theta_test': 27290.789924789326,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 104.19538426073386,\n",
       "    'KL': 111365.35495473912,\n",
       "    'KL_theta_train': 110351.19449763525,\n",
       "    'KL_theta_test': 27235.625114630217,\n",
       "    'size': 4},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 109772.65946646499,\n",
       "    'KL_theta_train': 108369.4151337684,\n",
       "    'KL_theta_test': 27012.889981289503,\n",
       "    'size': 0}}, {'FWD': {'nll': 99.5160033649108,\n",
       "    'KL': 282586.659466475,\n",
       "    'KL_theta_train': 109245.40910261171,\n",
       "    'KL_theta_test': 26904.460619834532,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 99.56312282887708,\n",
       "    'KL': 284710.64039201796,\n",
       "    'KL_theta_train': 108333.7106912637,\n",
       "    'KL_theta_test': 26964.48631308417,\n",
       "    'size': 1},\n",
       "   'SLOPE': {'nll': 99.84484928768055,\n",
       "    'KL': 279865.55463686254,\n",
       "    'KL_theta_train': 108573.14638345738,\n",
       "    'KL_theta_test': 26828.626167475402,\n",
       "    'size': 4}}, {'FWD': {'nll': 103.43611271876655,\n",
       "    'KL': 88977.31830183591,\n",
       "    'KL_theta_train': 108727.06844899274,\n",
       "    'KL_theta_test': 27640.291521464736,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.05062835249487,\n",
       "    'KL': 85611.32406979994,\n",
       "    'KL_theta_train': 108060.44164997424,\n",
       "    'KL_theta_test': 27139.63262181397,\n",
       "    'size': 3},\n",
       "   'SLOPE': {'nll': 99.54011828437028,\n",
       "    'KL': 83316.8309867528,\n",
       "    'KL_theta_train': 109213.34968119435,\n",
       "    'KL_theta_test': 26521.307271596783,\n",
       "    'size': 6}}, {'FWD': {'nll': 101.10370450818787,\n",
       "    'KL': 98523.7203537373,\n",
       "    'KL_theta_train': 107733.01158800573,\n",
       "    'KL_theta_test': 26805.958637428947,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 98731.80809119837,\n",
       "    'KL_theta_train': 108507.94795073336,\n",
       "    'KL_theta_test': 26996.98727605332,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.23154803758133,\n",
       "    'KL': 98744.40918150457,\n",
       "    'KL_theta_train': 107478.05332909615,\n",
       "    'KL_theta_test': 26955.03047417634,\n",
       "    'size': 2}}, {'FWD': {'nll': 99.82904396573547,\n",
       "    'KL': 90627.67109906246,\n",
       "    'KL_theta_train': 108537.7215572295,\n",
       "    'KL_theta_test': 27153.73589786889,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.12928074942816,\n",
       "    'KL': 90553.79787541417,\n",
       "    'KL_theta_train': 108462.88546969157,\n",
       "    'KL_theta_test': 26993.774457370684,\n",
       "    'size': 1},\n",
       "   'SLOPE': {'nll': 104.86291778036397,\n",
       "    'KL': 87629.5410997739,\n",
       "    'KL_theta_train': 109677.3682196943,\n",
       "    'KL_theta_test': 26540.914915436173,\n",
       "    'size': 7}}, {'FWD': {'nll': 102.67925588593727,\n",
       "    'KL': 101143.18275815314,\n",
       "    'KL_theta_train': 106976.50304136425,\n",
       "    'KL_theta_test': 27832.60077305036,\n",
       "    'size': 3},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 98340.82553158098,\n",
       "    'KL_theta_train': 108459.08978805598,\n",
       "    'KL_theta_test': 27007.449778797083,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 100.0,\n",
       "    'KL': 98340.82553158098,\n",
       "    'KL_theta_train': 108459.08978805598,\n",
       "    'KL_theta_test': 27007.449778797083,\n",
       "    'size': 0}}, {'FWD': {'nll': 100.78125137483546,\n",
       "    'KL': 86593.92263492457,\n",
       "    'KL_theta_train': 108330.73416142105,\n",
       "    'KL_theta_test': 26724.913594103913,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 87907.2365144721,\n",
       "    'KL_theta_train': 108361.04852993574,\n",
       "    'KL_theta_test': 27043.344856031184,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 99.796505562304,\n",
       "    'KL': 87995.55027978538,\n",
       "    'KL_theta_train': 108101.1376081764,\n",
       "    'KL_theta_test': 27086.52262220504,\n",
       "    'size': 1}}, {'FWD': {'nll': 99.81936709669613,\n",
       "    'KL': 70470.26650211232,\n",
       "    'KL_theta_train': 107571.62605107205,\n",
       "    'KL_theta_test': 27146.53547495163,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 70507.47655849166,\n",
       "    'KL_theta_train': 108404.26213980728,\n",
       "    'KL_theta_test': 27035.125405026636,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 99.97429658335665,\n",
       "    'KL': 70507.57985558533,\n",
       "    'KL_theta_train': 108339.6042266062,\n",
       "    'KL_theta_test': 27051.818766374647,\n",
       "    'size': 1}}, {'FWD': {'nll': 99.59513577891948,\n",
       "    'KL': 145394.32707558147,\n",
       "    'KL_theta_train': 108380.53090698551,\n",
       "    'KL_theta_test': 26996.530152148087,\n",
       "    'size': 2},\n",
       "   'LASSO': {'nll': 100.0,\n",
       "    'KL': 145745.08437722514,\n",
       "    'KL_theta_train': 108438.31772161684,\n",
       "    'KL_theta_test': 27021.133060010405,\n",
       "    'size': 0},\n",
       "   'SLOPE': {'nll': 99.554551258422,\n",
       "    'KL': 144940.9734613366,\n",
       "    'KL_theta_train': 108676.59570960628,\n",
       "    'KL_theta_test': 26991.289821494593,\n",
       "    'size': 2}}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9e79ca",
   "metadata": {},
   "source": [
    "### Cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c913312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running d: 20\n",
      "running d0: 1\n",
      "running rho: 0\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "from cupy.linalg import norm, inv\n",
    "# , dot\n",
    "from cupy.random import multivariate_normal\n",
    "from scipy.stats import poisson\n",
    "from simulation_runner_cupy import runner as sim_runner\n",
    "from simulation_runner_cupy import matrix_simulator \n",
    "import os\n",
    "import multiprocessing\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "pool = multiprocessing.Pool(os.cpu_count())\n",
    "# print(pool)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "# output = process_pool.starmap(f_sum, data)\n",
    "    results = {}\n",
    "    for rho in [0, 0.5, 0.8]:\n",
    "#         for d in simulation_settings:\n",
    "        for d in [20]:\n",
    "    #     d = 20\n",
    "            print('running d: {}'.format(d))\n",
    "            for di in simulation_settings[d]:\n",
    "                print('running d0: {}'.format(di))\n",
    "#                 for rho in [0, 0.5, 0.8]:\n",
    "                print('running rho: {}'.format(rho))\n",
    "                data = matrix_simulator(d, di, rho = rho)\n",
    "        #             print(data)\n",
    "                X_split = cp.asarray(np.vsplit(data[0], 100))\n",
    "                y_split = cp.asarray(np.split(data[1], 100))\n",
    "                theta_split = cp.asarray(np.split(data[2], 100))\n",
    "                map_args = [(X,y,theta, 'poisson') for X,y,theta in zip(X_split,y_split,theta_split)]\n",
    "#                 map_args = [{'X':X\n",
    "#                              ,'y':y\n",
    "#                              ,'theta':theta\n",
    "#                              , 'reg_type':'poisson'} for X,y,theta in zip(X_split,y_split,theta_split)]\n",
    "#                 print(map_args[0])\n",
    "                output = pool.starmap(sim_runner, map_args)\n",
    "#                 output = [sim_runner(**args) for args in map_args]\n",
    "                results[(d,di,rho)] = output\n",
    "                print('finished simulations of: {}'.format(d,di,rho))\n",
    "                with open('sim_dict_{()}'.format(rho), 'wb') as fp:\n",
    "                    pickle.dump(results, fp)\n",
    "    #             output = pool.map(sim_runner, map_args)\n",
    "        print(\"Finished Pooling rho: {}\".format(rho))\n",
    "            \n",
    "        \n",
    "#             print(zip(X_split,y_split,theta_split))\n",
    "#         result_forward = pool.starmap()\n",
    "#     print(d, simulation_settings[d])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c17d457c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'FWD': {'nll': 99.35713806900293,\n",
       "   'KL': 113327.65288423005,\n",
       "   'KL_theta_train': 107336.41232029392,\n",
       "   'KL_theta_test': 26939.28584059106,\n",
       "   'size': 2},\n",
       "  'LASSO': {'nll': 100.0,\n",
       "   'KL': 113795.92917122482,\n",
       "   'KL_theta_train': 108232.40193697,\n",
       "   'KL_theta_test': 26951.849003674917,\n",
       "   'size': 0},\n",
       "  'SLOPE': {'nll': 100.18733718367497,\n",
       "   'KL': 113982.65439084038,\n",
       "   'KL_theta_train': 108356.133129757,\n",
       "   'KL_theta_test': 26977.957944755915,\n",
       "   'size': 1}},\n",
       " {'FWD': {'nll': 100.69177586178331,\n",
       "   'KL': 115216.86434877384,\n",
       "   'KL_theta_train': 106929.0714084784,\n",
       "   'KL_theta_test': 27156.31033316313,\n",
       "   'size': 2},\n",
       "  'LASSO': {'nll': 100.08865434459278,\n",
       "   'KL': 119405.70620147133,\n",
       "   'KL_theta_train': 108014.48957653274,\n",
       "   'KL_theta_test': 27081.4053534604,\n",
       "   'size': 0},\n",
       "  'SLOPE': {'nll': 100.0,\n",
       "   'KL': 119068.93285354205,\n",
       "   'KL_theta_train': 108069.17305281028,\n",
       "   'KL_theta_test': 27014.03422566439,\n",
       "   'size': 0}},\n",
       " {'FWD': {'nll': 101.05881387225647,\n",
       "   'KL': 103489.24937908769,\n",
       "   'KL_theta_train': 107840.67266576277,\n",
       "   'KL_theta_test': 27390.05409703803,\n",
       "   'size': 2},\n",
       "  'LASSO': {'nll': 99.28958204746064,\n",
       "   'KL': 102327.5109708805,\n",
       "   'KL_theta_train': 108519.66887620019,\n",
       "   'KL_theta_test': 26676.442332091185,\n",
       "   'size': 1},\n",
       "  'SLOPE': {'nll': 100.0,\n",
       "   'KL': 103403.42327057893,\n",
       "   'KL_theta_train': 108358.6155593612,\n",
       "   'KL_theta_test': 27062.638876819758,\n",
       "   'size': 0}},\n",
       " {'FWD': {'nll': 100.34223065837857,\n",
       "   'KL': 133138.63776589124,\n",
       "   'KL_theta_train': 107480.14302964039,\n",
       "   'KL_theta_test': 26974.81728505946,\n",
       "   'size': 2},\n",
       "  'LASSO': {'nll': 99.34882814951918,\n",
       "   'KL': 132164.89666155854,\n",
       "   'KL_theta_train': 108147.50157240976,\n",
       "   'KL_theta_test': 26858.604165610603,\n",
       "   'size': 4},\n",
       "  'SLOPE': {'nll': 99.83914057335286,\n",
       "   'KL': 133598.00404549966,\n",
       "   'KL_theta_train': 108295.31641753785,\n",
       "   'KL_theta_test': 26967.170168701352,\n",
       "   'size': 1}},\n",
       " {'FWD': {'nll': 99.68414806620113,\n",
       "   'KL': 66596.59346782594,\n",
       "   'KL_theta_train': 107992.6474153721,\n",
       "   'KL_theta_test': 27211.33275942179,\n",
       "   'size': 2},\n",
       "  'LASSO': {'nll': 100.22496175166584,\n",
       "   'KL': 66588.78041697117,\n",
       "   'KL_theta_train': 108187.73834500433,\n",
       "   'KL_theta_test': 26992.421773154623,\n",
       "   'size': 3},\n",
       "  'SLOPE': {'nll': 100.0,\n",
       "   'KL': 66512.52875749629,\n",
       "   'KL_theta_train': 108069.44340709964,\n",
       "   'KL_theta_test': 27005.10002301925,\n",
       "   'size': 0}},\n",
       " {'FWD': {'nll': 108.36042075836171,\n",
       "   'KL': 178536.5865674263,\n",
       "   'KL_theta_train': 108711.40179349596,\n",
       "   'KL_theta_test': 26645.141356578282,\n",
       "   'size': 3},\n",
       "  'LASSO': {'nll': 99.72271656931198,\n",
       "   'KL': 176353.89567196005,\n",
       "   'KL_theta_train': 108264.67599544887,\n",
       "   'KL_theta_test': 27061.024883939514,\n",
       "   'size': 0},\n",
       "  'SLOPE': {'nll': 99.71389652632746,\n",
       "   'KL': 176353.89957808028,\n",
       "   'KL_theta_train': 108269.62277744099,\n",
       "   'KL_theta_test': 27062.049725717112,\n",
       "   'size': 0}},\n",
       " {'FWD': {'nll': 100.80527775578274,\n",
       "   'KL': 166944.88554835905,\n",
       "   'KL_theta_train': 108035.15929076776,\n",
       "   'KL_theta_test': 26972.153853107204,\n",
       "   'size': 2},\n",
       "  'LASSO': {'nll': 100.35820021269939,\n",
       "   'KL': 166699.76505106475,\n",
       "   'KL_theta_train': 108187.4586345175,\n",
       "   'KL_theta_test': 26945.719804090917,\n",
       "   'size': 2},\n",
       "  'SLOPE': {'nll': 101.18081361114675,\n",
       "   'KL': 167534.83396000494,\n",
       "   'KL_theta_train': 108848.88765558254,\n",
       "   'KL_theta_test': 27100.860554767238,\n",
       "   'size': 1}},\n",
       " {'FWD': {'nll': 99.99340909169356,\n",
       "   'KL': 79488.13399629005,\n",
       "   'KL_theta_train': 108130.15304333535,\n",
       "   'KL_theta_test': 27345.135833415803,\n",
       "   'size': 2},\n",
       "  'LASSO': {'nll': 103.96532897928307,\n",
       "   'KL': 82010.93697217997,\n",
       "   'KL_theta_train': 108132.36814063607,\n",
       "   'KL_theta_test': 28309.423413540262,\n",
       "   'size': 5},\n",
       "  'SLOPE': {'nll': 99.97463024381011,\n",
       "   'KL': 79287.60667399438,\n",
       "   'KL_theta_train': 108429.22158831435,\n",
       "   'KL_theta_test': 27111.209668194602,\n",
       "   'size': 1}}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results[(d, di, rho)] = [res for res in output]\n",
    "# results\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4697b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "301bba9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-5.78391609e-03, -4.03495220e-03, -4.18974836e-03, ...,\n",
       "          2.66413076e-03,  1.14657787e-03,  3.98668167e-03],\n",
       "        [ 2.08829305e-03,  3.11895185e-03,  2.12775132e-03, ...,\n",
       "         -1.07492684e-02,  2.94142482e-04, -2.96663166e-04],\n",
       "        [-1.63103498e-03,  5.23481844e-03, -9.67172344e-03, ...,\n",
       "          4.15401949e-03,  1.73961241e-04,  1.17674197e-03],\n",
       "        ...,\n",
       "        [ 2.53987048e-03,  1.80619388e-04, -7.26024073e-04, ...,\n",
       "         -1.10541895e-02, -5.31152890e-03, -5.14544308e-03],\n",
       "        [-1.46999783e-03,  2.38073217e-05,  5.94246903e-03, ...,\n",
       "          1.32458038e-04, -1.77938026e-03,  4.90695259e-03],\n",
       "        [-3.88378712e-03,  1.78927393e-03,  2.50552796e-04, ...,\n",
       "          2.00161648e-03,  4.07938619e-03, -8.57777341e-04]]),\n",
       " array([2, 4, 2, ..., 0, 1, 0]),\n",
       " array([1.00306099, 1.00326202, 0.9991098 , ..., 1.0004237 , 0.99905366,\n",
       "        0.99958739]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 20\n",
    "d0 = 5\n",
    "matrix_simulator(20, 5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52069446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de5756e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import poisson, norm, multivariate_normal\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import multi_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9d06a",
   "metadata": {},
   "source": [
    "# General funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f3f3180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pois_ll(X, y, beta):\n",
    "    Xb = np.dot(X,beta)\n",
    "    ll = np.dot(Xb,y) - np.sum(np.exp(Xb))\n",
    "    return ll\n",
    "\n",
    "def pois_KL(X, y, beta, theta = None):\n",
    "    Xb = np.dot(X,beta)\n",
    "    if theta is None:\n",
    "        rel = y\n",
    "    else:\n",
    "        rel = theta\n",
    "    kl_1 = np.dot(np.exp(rel),rel-Xb)\n",
    "    kl_2 = np.exp(rel) - np.exp(Xb)\n",
    "    KL = np.sum(kl_1-kl_2)\n",
    "    return KL\n",
    "\n",
    "def nb_ll(X, y, beta, alpha):\n",
    "    Xb = np.dot(X,beta)\n",
    "    ll = np.dot(y,Xb) - np.dot(np.ones(y.shape[0]) * alpha + y , np.log(np.exp(Xb) + alpha))\n",
    "    return np.sum(ll)\n",
    "\n",
    "def nb_KL(X, y, beta, alpha ,theta = None):\n",
    "    Xb = np.dot(X,beta)\n",
    "    if theta is None:\n",
    "        rel = y\n",
    "    else:\n",
    "        rel = theta\n",
    "    kl_1_par1 = np.log(np.exp(rel) / (np.exp(rel) + alpha))\n",
    "    kl_1_par2 = np.log(np.exp(Xb) / (np.exp(Xb) + alpha))\n",
    "    kl_1 = np.dot(exp(rel),kl_1_par1-kl_1_par2)\n",
    "    \n",
    "    kl_2 = np.log((np.exp(Xb) + alpha) / (np.exp(y) + alpha)) * alpha\n",
    "    \n",
    "    KL = np.sum(kl_1+kl_2)\n",
    "    return KL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4289202b",
   "metadata": {},
   "source": [
    "# IRLS and Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d77a916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#### IRLS Algorithm ####\n",
    "########################\n",
    "\n",
    "def IRLS(X, y, reg_type: ['poisson','nb']\n",
    "         , alpha = None, threshold = 0.01\n",
    "         , just_score = True):    \n",
    "    beta = np.zeros((X.shape[1]))\n",
    "    #### Distribution specifics ####\n",
    "    ## Poisson    \n",
    "    if reg_type == 'poisson':\n",
    "        ll_cur = pois_ll(X, y, beta)\n",
    "        W = np.diagflat(np.exp(np.dot(X,beta)))\n",
    "        D = np.diagflat(np.exp(np.dot(X,beta)))\n",
    "        z = np.dot(X,beta) + np.dot(inv(D),(y-np.exp(np.dot(X,beta))))\n",
    "        beta_est = multi_dot([inv(multi_dot([np.transpose(X),W,X])) , np.transpose(X), W, z])\n",
    "        ll_next = pois_ll(X, y, beta_est)\n",
    "        \n",
    "        # IRLS part\n",
    "        \n",
    "        while ll_next - ll_cur > threshold:\n",
    "            ll_cur = pois_ll(X, y, beta_est)\n",
    "            W = np.diagflat(np.exp(np.dot(X,beta_est)))\n",
    "            D = np.diagflat(np.exp(np.dot(X,beta_est)))\n",
    "            z = np.dot(X,beta_est) + np.dot(inv(D),(y-np.exp(np.dot(X,beta_est))))\n",
    "            beta_est = multi_dot([inv(multi_dot([np.transpose(X),W,X])) , np.transpose(X), W, z])\n",
    "            ll_next = pois_ll(X, y, beta_est)\n",
    "    ## NB        \n",
    "    if reg_type == 'nb':\n",
    "        ll_cur = nb_ll(X, y, beta, alpha)\n",
    "        W = np.diagflat(np.exp(np.dot(X,beta))/(1+(alpha**-1) * np.exp(np.dot(X,beta)))) # adjust to nb\n",
    "        D = np.diagflat(np.exp(np.dot(X,beta)))\n",
    "        z = np.dot(X,beta) + np.dot(inv(D),(y-np.exp(np.dot(X,beta))))\n",
    "        beta_est = np.dot(multi_dot([inv(multi_dot([np.transpose(X),W,X])) , np.transpose(X), W]), z)\n",
    "        ll_next = nb_ll(X, y, beta_est, alpha)\n",
    "        \n",
    "        # IRLS part\n",
    "        \n",
    "        while ll_next - ll_cur > threshold:\n",
    "            ll_cur = nb_ll(X, y, beta_est, alpha)\n",
    "            W = np.diagflat(np.exp(np.dot(X,beta_est))/(1+(alpha**-1) * np.exp(np.dot(X,beta_est)))) # adjust to nb\n",
    "            D = np.diagflat((np.exp(np.dot(X,beta_est))))\n",
    "            z = np.dot(X,beta_est) + np.dot(inv(D),(y-np.exp(np.dot(X,beta_est))))\n",
    "            beta_est = np.dot(multi_dot([inv(multi_dot([np.transpose(X),W,X])) , np.transpose(X), W]), z)\n",
    "            ll_next = nb_ll(X, y, beta_est, alpha)\n",
    "    \n",
    "    if just_score:\n",
    "        return ll_next\n",
    "    else:\n",
    "        return beta_est, ll_next, np.exp(np.dot(X,beta_est))\n",
    "\n",
    "###########################\n",
    "#### Forward Algorithm ####\n",
    "###########################\n",
    "\n",
    "def fwd(X, y, \n",
    "        reg_type: ['poisson','nb'], criteria: ['AIC','BIC','RIC']\n",
    "        , sel_feat = None, sel_dict = None, alpha = None):\n",
    "    \"\"\"\n",
    "    X, y, \n",
    "        reg_type: ['poisson','nb'], criteria: ['AIC','BIC','RIC']\n",
    "        , sel_feat = None, sel_dict = None, alpha = None\n",
    "    \"\"\"\n",
    "    if sel_feat is None:\n",
    "        sel_feat = []\n",
    "    if sel_dict is None:\n",
    "        sel_dict = {'features':[],'score': np.inf}\n",
    "#         feat_score =\n",
    "    for feature in [i for i in range(X.shape[1]) if i not in sel_feat]:\n",
    "#         print(sel_feat)\n",
    "#         print(feature)\n",
    "        in_feat = sel_feat+[feature]\n",
    "#         print(in_feat)\n",
    "#         print('currently testing:')\n",
    "#         print(in_feat)\n",
    "        # Criteria definition:\n",
    "        if criteria == 'AIC':\n",
    "            pen = len(in_feat)\n",
    "        if criteria == 'BIC':\n",
    "            pen = 0.5 * np.log(X.shape[0])* len(in_feat)\n",
    "        if criteria == 'RIC':\n",
    "            pen = np.log(X.shape[1])* len(in_feat)\n",
    "        if criteria == 'NLP': #NLP - Non-Linear penalty\n",
    "            pen = len(in_feat) * np.log(X.shape[1] * np.exp(1) / len(in_feat))\n",
    "        feat_score_next = -IRLS(X[:,in_feat],y,reg_type,alpha)\n",
    "        if feat_score_next < sel_dict['score']:\n",
    "            feat_score = feat_score_next + pen # taking the ll score\n",
    "            sel_dict['features'] = in_feat\n",
    "            sel_dict['score'] = feat_score\n",
    "            sel_dict['-ll']  = feat_score_next\n",
    "    return sel_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02d2370",
   "metadata": {},
   "source": [
    "# FISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a533c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "def pois_nll_grad(X,y,beta):\n",
    "    Xb = np.dot(X,beta).astype(np.float64)\n",
    "#     exp_ob = \n",
    "    nll_grad = np.dot(np.transpose(X),np.exp(Xb) - y)\n",
    "    return nll_grad\n",
    "\n",
    "def nb_nll_grad(X,y,beta,alpha):\n",
    "    Xb = np.dot(X,beta).astype(np.float64)\n",
    "    sec_factor = (np.exp(Xb) - y)/(np.exp(Xb) + alpha)\n",
    "    nll_grad = alpha * np.dot(np.transpose(X),sec_factor)\n",
    "    return nll_grad\n",
    "\n",
    "def L_pois(X,y):\n",
    "    eigs = LA.eigh(np.dot(np.transpose(X),X))[0]\n",
    "    idx = eigs.argsort()[::-1][0]  \n",
    "    eig_max = eigs[idx]\n",
    "    return np.mean(y) * eig_max\n",
    "    \n",
    "def L_nb(X,y,alpha):\n",
    "    eigs = LA.eigh(np.dot(np.transpose(X),X))[0]\n",
    "    idx = eigs.argsort()[::-1][0]  \n",
    "    eig_max = eigs[idx]\n",
    "    return (alpha + np.mean(y))/alpha * eig_max\n",
    "\n",
    "def prox(grad, beta, L, pen_vec):\n",
    "    prox_inp = beta - 1/L * grad\n",
    "    prox_out = np.maximum(np.abs(prox_inp) - pen_vec,0) * np.sign(prox_inp)\n",
    "    return prox_out\n",
    "\n",
    "def FISTA(X, y, pen_vec, \n",
    "          type: ['poisson', 'nb'],\n",
    "          iterations = 1000, is_ordered = True\n",
    "          ,alpha = None\n",
    "          ):\n",
    "    \"\"\"\n",
    "    X - Design matrix\n",
    "    y - Dependent variables\n",
    "    pen_vec - The penalty vector\n",
    "    type - The regression type - Poisson or NB\n",
    "    Iterations - Number of iterations \n",
    "    is_ordered - True\n",
    "    alpha - Only relevant if we use NB\n",
    "    \"\"\"\n",
    "    beta_start = np.zeros(X.shape[1], dtype = np.float64)\n",
    "    w_start = np.zeros(X.shape[1], dtype = np.float64)\n",
    "    delta_start = 1\n",
    "    for k in range(iterations):\n",
    "        if type == 'poisson':\n",
    "            grad = pois_nll_grad(X, y, beta_start)\n",
    "            L = L_pois(X,y)\n",
    "        elif type == 'nb':\n",
    "            grad = nb_nll_grad(X, y, beta_start,alpha)\n",
    "            L = L_nb(X,y,alpha)\n",
    "        ### Ordering for the thresholding ###\n",
    "        \n",
    "        if is_ordered:\n",
    "\n",
    "            indx = np.argsort(beta_start)[::-1]\n",
    "\n",
    "            beta_start = beta_start[indx]\n",
    "            ind_dict = {i: j for i,j in zip([*range(beta_start.shape[0])], indx)}\n",
    "        ### Starting the FISTA. Pay attention that we must sort grad according to indx\n",
    "        \n",
    "        w_next = prox(grad[indx], beta_start, L, pen_vec)\n",
    "        delta_next = (1 + np.sqrt(1 + 4*delta_start**2))/2\n",
    "        beta_next = w_next + ((delta_start - 1)/delta_next)*(w_next - w_start)\n",
    "        \n",
    "        ### Re-ordering again for calculating the gradient\n",
    "        \n",
    "        beta_start = np.zeros(beta_next.shape[0])\n",
    "        for origin_ind in ind_dict:\n",
    "            beta_start[origin_ind] = beta_next[ind_dict[origin_ind]]\n",
    "        delta_start = delta_next\n",
    "        w_start = w_next\n",
    "    \n",
    "#         if k % 100 == 0:\n",
    "#             print(k)\n",
    "#             print('beta_start: {}'.format(beta_start))\n",
    "#             print('beta_next: {}'.format(beta_next))\n",
    "#             if type == 'poisson':\n",
    "#                 print('nll :{}'.format(-pois_ll(X,y,beta_next)))\n",
    "#             elif type == 'nb':\n",
    "#                 print('nll :{}'.format(-nb_ll(X,y,beta_next)))\n",
    "    return beta_next, ind_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d327376f",
   "metadata": {},
   "source": [
    "# Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db53ae3e",
   "metadata": {},
   "source": [
    "## Simulations Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e82b608c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{20: [1, 2, 3, 4, 5, 6, 7, 10, 14, 18],\n",
       " 100: [5, 10, 15, 20, 25, 30, 35, 50, 70, 90],\n",
       " 200: [10, 20, 30, 40, 50, 60, 70, 100, 140, 180],\n",
       " 500: [10, 20, 30, 40, 50, 60, 70, 100, 140, 180],\n",
       " 1000: [10, 20, 30, 40, 50, 60, 70, 100, 140, 180]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho = [0,0.5,0.8]\n",
    "n = 200\n",
    "d = [20, 100, 200, 500, 1000]\n",
    "per = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.5, 0.7, 0.9]\n",
    "simulation_settings = {i: [] for i in d}\n",
    "\n",
    "def d_0(d, per, n):\n",
    "    d_0 = per*np.minimum(d,n)\n",
    "    return int(d_0)\n",
    "\n",
    "for di in d:\n",
    "#     print('d: {}'.format(di))\n",
    "    for peri in per:\n",
    "#         print('per: {}'.format(peri))\n",
    "#         print(d_0(di,peri,n))\n",
    "        simulation_settings[di].append(d_0(di,peri,n))\n",
    "# d_0(di,peri,n)\n",
    "simulation_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78548626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{20: [1, 2, 3, 4, 5, 6, 7, 10, 14, 18],\n",
       " 100: [5, 10, 15, 20, 25, 30, 35, 50, 70, 90],\n",
       " 200: [10, 20, 30, 40, 50, 60, 70, 100, 140, 180],\n",
       " 500: [10, 20, 30, 40, 50, 60, 70, 100, 140, 180],\n",
       " 1000: [10, 20, 30, 40, 50, 60, 70, 100, 140, 180]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulation_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5f982cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using penalty:AIC\n",
      "currently testing:\n",
      "[0]\n",
      "currently testing:\n",
      "[1]\n",
      "currently testing:\n",
      "[2]\n",
      "currently testing:\n",
      "[3]\n",
      "currently testing:\n",
      "[4]\n",
      "currently testing:\n",
      "[5]\n",
      "currently testing:\n",
      "[6]\n",
      "currently testing:\n",
      "[7]\n",
      "currently testing:\n",
      "[8]\n",
      "currently testing:\n",
      "[9]\n",
      "currently testing:\n",
      "[10]\n",
      "currently testing:\n",
      "[11]\n",
      "currently testing:\n",
      "[12]\n",
      "currently testing:\n",
      "[13]\n",
      "currently testing:\n",
      "[14]\n",
      "currently testing:\n",
      "[15]\n",
      "currently testing:\n",
      "[16]\n",
      "currently testing:\n",
      "[17]\n",
      "currently testing:\n",
      "[18]\n",
      "currently testing:\n",
      "[19]\n",
      "currently testing:\n",
      "[20]\n",
      "currently testing:\n",
      "[21]\n",
      "currently testing:\n",
      "[22]\n",
      "currently testing:\n",
      "[23]\n",
      "currently testing:\n",
      "[24]\n",
      "currently testing:\n",
      "[25]\n",
      "currently testing:\n",
      "[26]\n",
      "currently testing:\n",
      "[27]\n",
      "currently testing:\n",
      "[28]\n",
      "currently testing:\n",
      "[29]\n",
      "currently testing:\n",
      "[30]\n",
      "currently testing:\n",
      "[31]\n",
      "currently testing:\n",
      "[32]\n",
      "currently testing:\n",
      "[33]\n",
      "currently testing:\n",
      "[34]\n",
      "currently testing:\n",
      "[35]\n",
      "currently testing:\n",
      "[36]\n",
      "currently testing:\n",
      "[37]\n",
      "currently testing:\n",
      "[38]\n",
      "currently testing:\n",
      "[39]\n",
      "currently testing:\n",
      "[40]\n",
      "currently testing:\n",
      "[41]\n",
      "currently testing:\n",
      "[42]\n",
      "currently testing:\n",
      "[43]\n",
      "currently testing:\n",
      "[44]\n",
      "currently testing:\n",
      "[45]\n",
      "currently testing:\n",
      "[46]\n",
      "currently testing:\n",
      "[47]\n",
      "currently testing:\n",
      "[48]\n",
      "currently testing:\n",
      "[49]\n",
      "currently testing:\n",
      "[49, 0]\n",
      "currently testing:\n",
      "[49, 1]\n",
      "currently testing:\n",
      "[49, 2]\n",
      "currently testing:\n",
      "[49, 3]\n",
      "currently testing:\n",
      "[49, 4]\n",
      "currently testing:\n",
      "[49, 5]\n",
      "currently testing:\n",
      "[49, 6]\n",
      "currently testing:\n",
      "[49, 7]\n",
      "currently testing:\n",
      "[49, 8]\n",
      "currently testing:\n",
      "[49, 9]\n",
      "currently testing:\n",
      "[49, 10]\n",
      "currently testing:\n",
      "[49, 11]\n",
      "currently testing:\n",
      "[49, 12]\n",
      "currently testing:\n",
      "[49, 13]\n",
      "currently testing:\n",
      "[49, 14]\n",
      "currently testing:\n",
      "[49, 15]\n",
      "currently testing:\n",
      "[49, 16]\n",
      "currently testing:\n",
      "[49, 17]\n",
      "currently testing:\n",
      "[49, 18]\n",
      "currently testing:\n",
      "[49, 19]\n",
      "currently testing:\n",
      "[49, 20]\n",
      "currently testing:\n",
      "[49, 21]\n",
      "currently testing:\n",
      "[49, 22]\n",
      "currently testing:\n",
      "[49, 23]\n",
      "currently testing:\n",
      "[49, 24]\n",
      "currently testing:\n",
      "[49, 25]\n",
      "currently testing:\n",
      "[49, 26]\n",
      "currently testing:\n",
      "[49, 27]\n",
      "currently testing:\n",
      "[49, 28]\n",
      "currently testing:\n",
      "[49, 29]\n",
      "currently testing:\n",
      "[49, 30]\n",
      "currently testing:\n",
      "[49, 31]\n",
      "currently testing:\n",
      "[49, 32]\n",
      "currently testing:\n",
      "[49, 33]\n",
      "currently testing:\n",
      "[49, 34]\n",
      "currently testing:\n",
      "[49, 35]\n",
      "currently testing:\n",
      "[49, 36]\n",
      "currently testing:\n",
      "[49, 37]\n",
      "currently testing:\n",
      "[49, 38]\n",
      "currently testing:\n",
      "[49, 39]\n",
      "currently testing:\n",
      "[49, 40]\n",
      "currently testing:\n",
      "[49, 41]\n",
      "currently testing:\n",
      "[49, 42]\n",
      "currently testing:\n",
      "[49, 43]\n",
      "currently testing:\n",
      "[49, 44]\n",
      "currently testing:\n",
      "[49, 45]\n",
      "currently testing:\n",
      "[49, 46]\n",
      "currently testing:\n",
      "[49, 47]\n",
      "currently testing:\n",
      "[49, 48]\n",
      "using penalty:BIC\n",
      "currently testing:\n",
      "[0]\n",
      "currently testing:\n",
      "[1]\n",
      "currently testing:\n",
      "[2]\n",
      "currently testing:\n",
      "[3]\n",
      "currently testing:\n",
      "[4]\n",
      "currently testing:\n",
      "[5]\n",
      "currently testing:\n",
      "[6]\n",
      "currently testing:\n",
      "[7]\n",
      "currently testing:\n",
      "[8]\n",
      "currently testing:\n",
      "[9]\n",
      "currently testing:\n",
      "[10]\n",
      "currently testing:\n",
      "[11]\n",
      "currently testing:\n",
      "[12]\n",
      "currently testing:\n",
      "[13]\n",
      "currently testing:\n",
      "[14]\n",
      "currently testing:\n",
      "[15]\n",
      "currently testing:\n",
      "[16]\n",
      "currently testing:\n",
      "[17]\n",
      "currently testing:\n",
      "[18]\n",
      "currently testing:\n",
      "[19]\n",
      "currently testing:\n",
      "[20]\n",
      "currently testing:\n",
      "[21]\n",
      "currently testing:\n",
      "[22]\n",
      "currently testing:\n",
      "[23]\n",
      "currently testing:\n",
      "[24]\n",
      "currently testing:\n",
      "[25]\n",
      "currently testing:\n",
      "[26]\n",
      "currently testing:\n",
      "[27]\n",
      "currently testing:\n",
      "[28]\n",
      "currently testing:\n",
      "[29]\n",
      "currently testing:\n",
      "[30]\n",
      "currently testing:\n",
      "[31]\n",
      "currently testing:\n",
      "[32]\n",
      "currently testing:\n",
      "[33]\n",
      "currently testing:\n",
      "[34]\n",
      "currently testing:\n",
      "[35]\n",
      "currently testing:\n",
      "[36]\n",
      "currently testing:\n",
      "[37]\n",
      "currently testing:\n",
      "[38]\n",
      "currently testing:\n",
      "[39]\n",
      "currently testing:\n",
      "[40]\n",
      "currently testing:\n",
      "[41]\n",
      "currently testing:\n",
      "[42]\n",
      "currently testing:\n",
      "[43]\n",
      "currently testing:\n",
      "[44]\n",
      "currently testing:\n",
      "[45]\n",
      "currently testing:\n",
      "[46]\n",
      "currently testing:\n",
      "[47]\n",
      "currently testing:\n",
      "[48]\n",
      "currently testing:\n",
      "[49]\n",
      "currently testing:\n",
      "[49, 0]\n",
      "currently testing:\n",
      "[49, 1]\n",
      "currently testing:\n",
      "[49, 2]\n",
      "currently testing:\n",
      "[49, 3]\n",
      "currently testing:\n",
      "[49, 4]\n",
      "currently testing:\n",
      "[49, 5]\n",
      "currently testing:\n",
      "[49, 6]\n",
      "currently testing:\n",
      "[49, 7]\n",
      "currently testing:\n",
      "[49, 8]\n",
      "currently testing:\n",
      "[49, 9]\n",
      "currently testing:\n",
      "[49, 10]\n",
      "currently testing:\n",
      "[49, 11]\n",
      "currently testing:\n",
      "[49, 12]\n",
      "currently testing:\n",
      "[49, 13]\n",
      "currently testing:\n",
      "[49, 14]\n",
      "currently testing:\n",
      "[49, 15]\n",
      "currently testing:\n",
      "[49, 16]\n",
      "currently testing:\n",
      "[49, 17]\n",
      "currently testing:\n",
      "[49, 18]\n",
      "currently testing:\n",
      "[49, 19]\n",
      "currently testing:\n",
      "[49, 20]\n",
      "currently testing:\n",
      "[49, 21]\n",
      "currently testing:\n",
      "[49, 22]\n",
      "currently testing:\n",
      "[49, 23]\n",
      "currently testing:\n",
      "[49, 24]\n",
      "currently testing:\n",
      "[49, 25]\n",
      "currently testing:\n",
      "[49, 26]\n",
      "currently testing:\n",
      "[49, 27]\n",
      "currently testing:\n",
      "[49, 28]\n",
      "currently testing:\n",
      "[49, 29]\n",
      "currently testing:\n",
      "[49, 30]\n",
      "currently testing:\n",
      "[49, 31]\n",
      "currently testing:\n",
      "[49, 32]\n",
      "currently testing:\n",
      "[49, 33]\n",
      "currently testing:\n",
      "[49, 34]\n",
      "currently testing:\n",
      "[49, 35]\n",
      "currently testing:\n",
      "[49, 36]\n",
      "currently testing:\n",
      "[49, 37]\n",
      "currently testing:\n",
      "[49, 38]\n",
      "currently testing:\n",
      "[49, 39]\n",
      "currently testing:\n",
      "[49, 40]\n",
      "currently testing:\n",
      "[49, 41]\n",
      "currently testing:\n",
      "[49, 42]\n",
      "currently testing:\n",
      "[49, 43]\n",
      "currently testing:\n",
      "[49, 44]\n",
      "currently testing:\n",
      "[49, 45]\n",
      "currently testing:\n",
      "[49, 46]\n",
      "currently testing:\n",
      "[49, 47]\n",
      "currently testing:\n",
      "[49, 48]\n",
      "using penalty:RIC\n",
      "currently testing:\n",
      "[0]\n",
      "currently testing:\n",
      "[1]\n",
      "currently testing:\n",
      "[2]\n",
      "currently testing:\n",
      "[3]\n",
      "currently testing:\n",
      "[4]\n",
      "currently testing:\n",
      "[5]\n",
      "currently testing:\n",
      "[6]\n",
      "currently testing:\n",
      "[7]\n",
      "currently testing:\n",
      "[8]\n",
      "currently testing:\n",
      "[9]\n",
      "currently testing:\n",
      "[10]\n",
      "currently testing:\n",
      "[11]\n",
      "currently testing:\n",
      "[12]\n",
      "currently testing:\n",
      "[13]\n",
      "currently testing:\n",
      "[14]\n",
      "currently testing:\n",
      "[15]\n",
      "currently testing:\n",
      "[16]\n",
      "currently testing:\n",
      "[17]\n",
      "currently testing:\n",
      "[18]\n",
      "currently testing:\n",
      "[19]\n",
      "currently testing:\n",
      "[20]\n",
      "currently testing:\n",
      "[21]\n",
      "currently testing:\n",
      "[22]\n",
      "currently testing:\n",
      "[23]\n",
      "currently testing:\n",
      "[24]\n",
      "currently testing:\n",
      "[25]\n",
      "currently testing:\n",
      "[26]\n",
      "currently testing:\n",
      "[27]\n",
      "currently testing:\n",
      "[28]\n",
      "currently testing:\n",
      "[29]\n",
      "currently testing:\n",
      "[30]\n",
      "currently testing:\n",
      "[31]\n",
      "currently testing:\n",
      "[32]\n",
      "currently testing:\n",
      "[33]\n",
      "currently testing:\n",
      "[34]\n",
      "currently testing:\n",
      "[35]\n",
      "currently testing:\n",
      "[36]\n",
      "currently testing:\n",
      "[37]\n",
      "currently testing:\n",
      "[38]\n",
      "currently testing:\n",
      "[39]\n",
      "currently testing:\n",
      "[40]\n",
      "currently testing:\n",
      "[41]\n",
      "currently testing:\n",
      "[42]\n",
      "currently testing:\n",
      "[43]\n",
      "currently testing:\n",
      "[44]\n",
      "currently testing:\n",
      "[45]\n",
      "currently testing:\n",
      "[46]\n",
      "currently testing:\n",
      "[47]\n",
      "currently testing:\n",
      "[48]\n",
      "currently testing:\n",
      "[49]\n",
      "currently testing:\n",
      "[49, 0]\n",
      "currently testing:\n",
      "[49, 1]\n",
      "currently testing:\n",
      "[49, 2]\n",
      "currently testing:\n",
      "[49, 3]\n",
      "currently testing:\n",
      "[49, 4]\n",
      "currently testing:\n",
      "[49, 5]\n",
      "currently testing:\n",
      "[49, 6]\n",
      "currently testing:\n",
      "[49, 7]\n",
      "currently testing:\n",
      "[49, 8]\n",
      "currently testing:\n",
      "[49, 9]\n",
      "currently testing:\n",
      "[49, 10]\n",
      "currently testing:\n",
      "[49, 11]\n",
      "currently testing:\n",
      "[49, 12]\n",
      "currently testing:\n",
      "[49, 13]\n",
      "currently testing:\n",
      "[49, 14]\n",
      "currently testing:\n",
      "[49, 15]\n",
      "currently testing:\n",
      "[49, 16]\n",
      "currently testing:\n",
      "[49, 17]\n",
      "currently testing:\n",
      "[49, 18]\n",
      "currently testing:\n",
      "[49, 19]\n",
      "currently testing:\n",
      "[49, 20]\n",
      "currently testing:\n",
      "[49, 21]\n",
      "currently testing:\n",
      "[49, 22]\n",
      "currently testing:\n",
      "[49, 23]\n",
      "currently testing:\n",
      "[49, 24]\n",
      "currently testing:\n",
      "[49, 25]\n",
      "currently testing:\n",
      "[49, 26]\n",
      "currently testing:\n",
      "[49, 27]\n",
      "currently testing:\n",
      "[49, 28]\n",
      "currently testing:\n",
      "[49, 29]\n",
      "currently testing:\n",
      "[49, 30]\n",
      "currently testing:\n",
      "[49, 31]\n",
      "currently testing:\n",
      "[49, 32]\n",
      "currently testing:\n",
      "[49, 33]\n",
      "currently testing:\n",
      "[49, 34]\n",
      "currently testing:\n",
      "[49, 35]\n",
      "currently testing:\n",
      "[49, 36]\n",
      "currently testing:\n",
      "[49, 37]\n",
      "currently testing:\n",
      "[49, 38]\n",
      "currently testing:\n",
      "[49, 39]\n",
      "currently testing:\n",
      "[49, 40]\n",
      "currently testing:\n",
      "[49, 41]\n",
      "currently testing:\n",
      "[49, 42]\n",
      "currently testing:\n",
      "[49, 43]\n",
      "currently testing:\n",
      "[49, 44]\n",
      "currently testing:\n",
      "[49, 45]\n",
      "currently testing:\n",
      "[49, 46]\n",
      "currently testing:\n",
      "[49, 47]\n",
      "currently testing:\n",
      "[49, 48]\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "score_start = np.inf\n",
    "final_scores = {'AIC': {}, 'BIC': {} ,'RIC': {}}\n",
    "for crit in ['AIC','BIC','RIC']:\n",
    "    print('using penalty:{}'.format(crit))\n",
    "#     criteria = crit\n",
    "    score_start = np.inf\n",
    "    sel_dict = fwd(X[:300,:],y[:300],'poisson', crit)\n",
    "#     print(sel_dict)\n",
    "    counter = 0\n",
    "    while score_start > sel_dict['score']:\n",
    "        counter +=1\n",
    "        score_start = sel_dict['score']\n",
    "        sel_dict = fwd(X[:300,:],y[:300],'poisson',crit, sel_feat = sel_dict['features'], sel_dict = sel_dict)\n",
    "#         print('finished')\n",
    "#         print(counter)\n",
    "#     print('Results')\n",
    "#     print(sel_dict)\n",
    "    final_scores[crit] = sel_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2be0c930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AIC',\n",
       "  {'features': [49, 48],\n",
       "   'score': 303.6656246224452,\n",
       "   '-ll': 299.6656246224452}),\n",
       " ('BIC',\n",
       "  {'features': [49, 48],\n",
       "   'score': 311.0731895717576,\n",
       "   '-ll': 299.6656246224452}),\n",
       " ('RIC',\n",
       "  {'features': [49, 48],\n",
       "   'score': 315.31371664415775,\n",
       "   '-ll': 299.6656246224452})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_scores.items()\n",
    "sorted(final_scores.items(), key=lambda item: item[1]['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc14845d",
   "metadata": {},
   "source": [
    "## Simluations Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c993bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "from numpy.random import multivariate_normal\n",
    "import multiprocessing\n",
    "import itertools\n",
    "\n",
    "##### What is left?\n",
    "#### 1. KL - Over y test and Theta\n",
    "#### 2. Model Size\n",
    "\n",
    "def runner(X, y, theta\n",
    "           , reg_type: ['poisson','nb']\n",
    "#            , model_type: ['fwd','LASSO','SLOPE']\n",
    "           , pen_coef = np.exp(np.arange(-40,40,0.3))):\n",
    "    \n",
    "    # Creating train and test sets\n",
    "    d = X.shape[1]\n",
    "    (X_train, y_train, theta_train), (X_test, y_test, theta_test) = train_test_allocator(X, y, theta)\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    \n",
    "    model_scores_dict = {'FWD':{'nll':0,'KL':0, 'KL_theta_train':0, 'KL_theta_test':0, 'size':0}\n",
    "                         ,'LASSO':{'nll':0,'KL':0, 'KL_theta_train':0, 'KL_theta_test':0, 'size':0}\n",
    "                         ,'SLOPE':{'nll':0,'KL':0, 'KL_theta_train':0, 'KL_theta_test':0, 'size':0}}\n",
    "    # Creating k-folds #\n",
    "    indices = np.array([*range(X_train.shape[0])])\n",
    "    k_folds_inds = np.split(indices,5)\n",
    "    \n",
    "    #####################\n",
    "    # Forward selection #\n",
    "    #####################\n",
    "    print('Starting Forward Selection')\n",
    "    \n",
    "#     final_scores = {'AIC': {}, 'BIC': {} ,'RIC': {}}\n",
    "    folds_score = {'AIC': 0, 'BIC': 0 ,'RIC': 0, 'NLP':0}\n",
    "    # K-fold part\n",
    "\n",
    "    for crit in ['AIC','BIC','RIC','NLP']:\n",
    "        print('using penalty:{}'.format(crit))\n",
    "        score_start = np.inf\n",
    "#         folds_score = {'AIC': 0, 'BIC': 0 ,'RIC': 0}\n",
    "        for oos_fold in k_folds_inds:\n",
    "            fit_folds = [i for i in indices if i not in oos_fold]\n",
    "            # val set\n",
    "            val_set = X_train[oos_fold,:]\n",
    "            val_y = y_train[oos_fold]\n",
    "            # fit set\n",
    "            fit_set = X_train[fit_folds,:]        \n",
    "            fit_y = y_train[fit_folds]\n",
    "            sel_dict = fwd(X = fit_set,y = fit_y,reg_type = reg_type, criteria = crit)\n",
    "            while score_start > sel_dict['score']:\n",
    "#                 counter +=1\n",
    "                score_start = sel_dict['score']\n",
    "                sel_dict = fwd(fit_set,fit_y, reg_type,crit\n",
    "                               , sel_feat = sel_dict['features'], sel_dict = sel_dict)\n",
    "            \n",
    "            selected_features = sel_dict['features']\n",
    "            beta_est_vals = IRLS(fit_set[:,selected_features], fit_y, reg_type, just_score = False)[0]\n",
    "            beta_est = np.zeros(d)\n",
    "            beta_est[selected_features] = beta_est_vals\n",
    "            print(beta_est)\n",
    "            if reg_type == 'poisson':\n",
    "                ll = pois_ll(val_set, val_y, beta_est)\n",
    "            elif reg_type == 'nb':\n",
    "                ll = nb_ll(val_set, val_y\n",
    "                             , beta_est, alpha)\n",
    "            folds_score[crit] += ll\n",
    "            print(folds_score)\n",
    "        folds_score[crit] = folds_score[crit]/5\n",
    "#         final_scores[crit] = folds_score[crit]\n",
    "    \n",
    "    # Taking the best option:\n",
    "    \n",
    "    print(sorted(folds_score.items(), key=lambda item: item))\n",
    "    best_score_crit = sorted(folds_score.items(), key=lambda item: item[1])[0][0]\n",
    "    print('cv selection: {}'.format(best_score_crit))\n",
    "    # Fitting over entire train set:\n",
    "    \n",
    "    score_start = np.inf\n",
    "    sel_dict = fwd(X_train, y_train, reg_type, best_score_crit)\n",
    "#     counter = 0\n",
    "    while score_start > sel_dict['score']:\n",
    "#         counter +=1\n",
    "        score_start = sel_dict['score']\n",
    "        sel_dict = fwd(X_train, y_train, reg_type, best_score_crit\n",
    "                       , sel_feat = sel_dict['features'], sel_dict = sel_dict)\n",
    "    final_feautre_set = sel_dict['features']\n",
    "    \n",
    "    final_beta_vals = IRLS(X_train[:,final_feautre_set], y_train, reg_type, just_score = False)[0]\n",
    "    final_beta_est = np.zeros(d)\n",
    "    final_beta_est[final_feautre_set] = final_beta_vals\n",
    "    # Testing over the test set:\n",
    "    \n",
    "    if reg_type == 'poisson':\n",
    "        nll = -pois_ll( X_test, y_test, final_beta_est)\n",
    "        KL = pois_KL( X_test, y_test, final_beta_est, theta = None)\n",
    "        KL_theta_train = pois_KL( X_train, y_train, final_beta_est, theta = theta_train)\n",
    "        KL_theta_test = pois_KL( X_test, y_test, final_beta_est, theta = theta_test)\n",
    "    elif reg_type == 'nb':\n",
    "        nll = -nb_ll( X_test, y_test\n",
    "                     , final_beta_est, alpha)\n",
    "        KL = nb_KL( X_test, y_test\n",
    "                     , final_beta_est, alpha, theta = None)\n",
    "        KL_theta_train = nb_KL( X_train, y_train\n",
    "                     , final_beta_est, alpha, theta = theta_train)\n",
    "        KL_theta_test = nb_KL( X_test, y_test\n",
    "                     , final_beta_est, alpha, theta = theta_test)\n",
    "    \n",
    "    model_scores_dict['FWD']['nll'] += nll\n",
    "    model_scores_dict['FWD']['KL'] += KL\n",
    "    model_scores_dict['FWD']['KL_theta_train'] += KL_theta_train\n",
    "    model_scores_dict['FWD']['KL_theta_test'] += KL_theta_test\n",
    "    model_scores_dict['FWD']['size'] += len(final_feautre_set)\n",
    "   \n",
    "    #####################\n",
    "    ### LASSO & SLOPE ###\n",
    "    #####################\n",
    "    \n",
    "    print(\"Starting LASSO & SLOPE\")\n",
    "    \n",
    "#     d = X.shape[1]\n",
    "    pen_vec_LASSO = np.ones(d) * np.sqrt(2 * np.log(d))\n",
    "    print(pen_vec_LASSO)\n",
    "    pen_vec_SLOPE = np.array([np.sqrt(np.log(2*d/(j+1))) for j in range(d)])\n",
    "    print(pen_vec_SLOPE)\n",
    "    \n",
    "    \n",
    "    pen_coef_results = {'SLOPE':{},'LASSO':{}}\n",
    "    for C in pen_coef:\n",
    "        SLOPE_cv_score = 0\n",
    "        LASSO_cv_score = 0\n",
    "        print('testing C: {}'.format(C))\n",
    "        for oos_fold in k_folds_inds:\n",
    "            fit_folds = [i for i in indices if i not in oos_fold]\n",
    "            # val set\n",
    "            val_set = X_train[oos_fold,:]\n",
    "            val_y = y_train[oos_fold]\n",
    "            # fit set\n",
    "            fit_set = X_train[fit_folds,:]        \n",
    "            fit_y = y_train[fit_folds]\n",
    "            if reg_type == 'poisson':\n",
    "                # SLOPE\n",
    "                SLOPE_cv_beta = FISTA(fit_set,fit_y,C*pen_vec_SLOPE,reg_type)[0]\n",
    "                SLOPE_cv_score += -pois_ll(val_set, val_y, SLOPE_cv_beta)\n",
    "                \n",
    "                # LASSO\n",
    "                LASSO_cv_beta = FISTA(fit_set,fit_y,C*pen_vec_LASSO,reg_type)[0]\n",
    "                LASSO_cv_score += -pois_ll(val_set, val_y, LASSO_cv_beta)\n",
    "                \n",
    "            elif reg_type == 'nb':\n",
    "                # SLOPE\n",
    "                SLOPE_cv_beta = FISTA(fit_set,fit_y,C*pen_vec_SLOPE,reg_type, alpha = alpha)[0]\n",
    "                SLOPE_cv_score += -nb_ll(val_set, val_y, SLOPE_cv_beta, alpha = alpha)\n",
    "                # LASSO\n",
    "                LASSO_cv_beta = FISTA(fit_set,fit_y,C*pen_vec_LASSO,reg_type, alpha = alpha)[0]\n",
    "                LASSO_cv_score += -nb_ll(val_set, val_y, LASSO_cv_beta, alpha = alpha)\n",
    "        \n",
    "        pen_coef_results['SLOPE'][C] = SLOPE_cv_score/5\n",
    "        pen_coef_results['LASSO'][C] = LASSO_cv_score/5\n",
    "    \n",
    "    # Finding the best penalty value\n",
    "    \n",
    "    # Eliminating nans #\n",
    "    \n",
    "    pen_coef_results['SLOPE'] = {pen: score for pen, score in pen_coef_results['SLOPE'].items() if np.isnan(score) == False}\n",
    "    pen_coef_results['LASSO'] = {pen: score for pen, score in pen_coef_results['LASSO'].items() if np.isnan(score) == False}\n",
    "        \n",
    "    print('SLOPE result')\n",
    "    print(pen_coef_results['SLOPE'].items())\n",
    "    print({key: val for key, val in sorted(pen_coef_results['SLOPE'].items(), key=lambda item: item[1])})\n",
    "    print('LASSO result')\n",
    "    print(pen_coef_results['LASSO'].items())\n",
    "    print({key: val for key, val in sorted(pen_coef_results['LASSO'].items(), key=lambda item: item[1])})\n",
    "    SLOPE_sel_pen = sorted(pen_coef_results['SLOPE'].items(), key=lambda item: item[1])[0][0]\n",
    "    LASSO_sel_pen = sorted(pen_coef_results['LASSO'].items(), key=lambda item: item[1])[0][0]\n",
    "    \n",
    "    \n",
    "    # Fitting and testing over the best penalty value:\n",
    "    if reg_type == 'poisson':\n",
    "#         Fit\n",
    "        SLOPE_final_beta = FISTA(fit_set,fit_y,SLOPE_sel_pen*pen_vec_SLOPE,reg_type)[0]\n",
    "        SLOPE_final_beta_size = len(SLOPE_final_beta[SLOPE_final_beta > 0])\n",
    "        \n",
    "        LASSO_final_beta = FISTA(fit_set,fit_y,LASSO_sel_pen*pen_vec_LASSO,reg_type)[0]\n",
    "        LASSO_final_beta_size = len(LASSO_final_beta[LASSO_final_beta > 0])\n",
    "        \n",
    "#        eval SLOPE\n",
    "        SLOPE_nll = -pois_ll( X_test, y_test, SLOPE_final_beta)\n",
    "        SLOPE_KL = pois_KL( X_test, y_test, SLOPE_final_beta, theta = None)\n",
    "        SLOPE_KL_theta_train = pois_KL( X_train, y_train, SLOPE_final_beta, theta = theta_train)\n",
    "        SLOPE_KL_theta_test = pois_KL( X_test, y_test, SLOPE_final_beta, theta = theta_test)\n",
    "        \n",
    "#         eval LASSO\n",
    "        LASSO_nll = -pois_ll( X_test, y_test, LASSO_final_beta)\n",
    "        LASSO_KL = pois_KL( X_test, y_test, LASSO_final_beta, theta = None)\n",
    "        LASSO_KL_theta_train = pois_KL(X_train, y_train, LASSO_final_beta, theta = theta_train)\n",
    "        LASSO_KL_theta_test = pois_KL(X_test, y_test, LASSO_final_beta, theta = theta_test)\n",
    "    \n",
    "    elif reg_type == 'nb':\n",
    "#         Fit\n",
    "        SLOPE_final_beta = FISTA(fit_set,fit_y,SLOPE_sel_pen*pen_vec_SLOPE,reg_type, alpha = alpha)[0]\n",
    "        LASSO_final_beta = FISTA(fit_set,fit_y,LASSO_sel_pen*pen_vec_LASSO,reg_type, alpha = alpha)[0]\n",
    "#        eval\n",
    "        #        eval SLOPE\n",
    "        SLOPE_nll = -nb_ll( X_test, y_test, SLOPE_final_beta)\n",
    "        SLOPE_KL = nb_KL( X_test, y_test, SLOPE_final_beta, theta = None)\n",
    "        SLOPE_KL_theta_train = nb_KL( X_train, y_train, SLOPE_final_beta, theta = theta_train)\n",
    "        SLOPE_KL_theta_test = pois_KL( X_test, y_test, SLOPE_final_beta, theta = theta_test)\n",
    "        \n",
    "#         eval LASSO\n",
    "        LASSO_nll = -nb_ll( X_test, y_test, LASSO_final_beta)\n",
    "        LASSO_KL = nb_KL( X_test, y_test, LASSO_final_beta, theta = None)\n",
    "        LASSO_KL_theta_train = nb_KL( X_train, y_train, LASSO_final_beta, theta = theta_train)\n",
    "        LASSO_KL_theta_test = nb_KL( X_test, y_test, LASSO_final_beta, theta = theta_test)\n",
    "    \n",
    "    \n",
    "    model_scores_dict['SLOPE']['nll'] += SLOPE_nll\n",
    "    model_scores_dict['SLOPE']['KL'] += SLOPE_KL\n",
    "    model_scores_dict['SLOPE']['KL_theta_train'] += SLOPE_KL_theta_train\n",
    "    model_scores_dict['SLOPE']['KL_theta_test'] += SLOPE_KL_theta_test\n",
    "    model_scores_dict['SLOPE']['size'] += SLOPE_final_beta_size\n",
    "    \n",
    "    model_scores_dict['LASSO']['nll'] += LASSO_nll\n",
    "    model_scores_dict['LASSO']['KL'] += LASSO_KL\n",
    "    model_scores_dict['LASSO']['KL_theta_train'] += LASSO_KL_theta_train\n",
    "    model_scores_dict['LASSO']['KL_theta_test'] += LASSO_KL_theta_test\n",
    "    model_scores_dict['LASSO']['size'] += LASSO_final_beta_size\n",
    "    \n",
    "    print('Finished all')\n",
    "    \n",
    "    return model_scores_dict\n",
    "\n",
    "def train_test_allocator(X, y, theta, n_test = 100):\n",
    "    X_inds = [*range(X.shape[0])]\n",
    "    test_sample = np.random.choice(X_inds, 100,replace = False)\n",
    "    train_sample = [i for i in X_inds if i not in test_sample]\n",
    "    train_set = (X[train_sample], y[train_sample], theta[train_sample])\n",
    "    test_set = (X[test_sample], y[test_sample], theta[test_sample])\n",
    "    return train_set, test_set\n",
    "\n",
    "def matrix_simulator(d, d0\n",
    "                     , rho\n",
    "                     , beta_set = [0.5, -0.5, 0.6, -0.6]\n",
    "                     , n = 300\n",
    "                     , sim_num = 100):\n",
    "    \n",
    "    means = np.zeros(d)\n",
    "    if rho == 0:\n",
    "        conv_mat = np.diagflat(np.ones(d))\n",
    "    else:\n",
    "        conv_mat = cov_creator(d, rho)\n",
    "    X = multivariate_normal(means, conv_mat, size = n*sim_num)\n",
    "    X = X/norm(X, axis = 0)\n",
    "    \n",
    "    beta = beta_creator(d, d0, beta_set)\n",
    "    theta = np.exp(np.dot(X,beta))\n",
    "    y = poisson.rvs(theta) \n",
    "    return X, y, theta\n",
    "    \n",
    "def cov_creator(d, rho):\n",
    "    arr = np.zeros(d)\n",
    "    for i in range(d):\n",
    "        arr[i] = rho**((i+1)-1)\n",
    "    cov = np.zeros((d,d))\n",
    "    for i in range(d):\n",
    "        cov[i,:] = np.concatenate([arr[:i+1][::-1],arr[1:d-i]])\n",
    "    return cov\n",
    "\n",
    "def beta_creator(d, d0, beta_set = [0.5, -0.5, 0.6, -0.6]):\n",
    "    b_d0 = np.random.choice(beta_set, d)\n",
    "    zero_inds = np.random.choice([*range(d)],d-d0,replace = False)\n",
    "    b_d0[zero_inds] = 0\n",
    "    return b_d0\n",
    "\n",
    "    \n",
    "# def mp_apply():\n",
    "# for d in simulation_settings:\n",
    "#     # Simulate 100 matrices of 300 X d for each d_0\n",
    "#     # And generate the dpendent variable\n",
    "#     np."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2736ee9b",
   "metadata": {},
   "source": [
    "## Simulation Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c06d963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<multiprocessing.pool.Pool state=RUN pool_size=8>\n",
      "running d0: 1\n",
      "running rho: 0\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# pool = multiprocessing.Pool(os.cpu_count())\n",
    "# print(pool)\n",
    "# if __name__ == '__main__':\n",
    "# # output = process_pool.starmap(f_sum, data)\n",
    "# # results = {}\n",
    "# # # for d in simulation_settings:\n",
    "#     d = 20\n",
    "#     for di in simulation_settings[d][:1]:\n",
    "#         print('running d0: {}'.format(di))\n",
    "#         for rho in [0, 0.5, 0.8][:1]:\n",
    "#             print('running rho: {}'.format(rho))\n",
    "#             data = matrix_simulator(d, di, rho = rho, sim_num = 8)\n",
    "#     #             print(data)\n",
    "#             X_split = np.vsplit(data[0], 8)\n",
    "#             y_split = np.split(data[1], 8)\n",
    "#             theta_split = np.split(data[2], 8)\n",
    "#             map_args = [(X,y,theta, 'poisson') for X,y,theta in zip(X_split,y_split,theta_split)]\n",
    "#     #         print(map_args[0])\n",
    "# #             output = pool.starmap(runner, map_args)\n",
    "#             output = pool.map(runner, map_args)\n",
    "#             print(\"Finished Pooling\")\n",
    "        \n",
    "# #             print(zip(X_split,y_split,theta_split))\n",
    "# #         result_forward = pool.starmap()\n",
    "# #     print(d, simulation_settings[d])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd6f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<multiprocessing.pool.Pool state=RUN pool_size=8>\n",
      "running d: 20\n",
      "running d0: 1\n",
      "running rho: 0\n",
      "finished simulations of: 20\n",
      "running rho: 0.5\n",
      "finished simulations of: 20\n",
      "running rho: 0.8\n",
      "finished simulations of: 20\n",
      "running d0: 2\n",
      "running rho: 0\n",
      "finished simulations of: 20\n",
      "running rho: 0.5\n",
      "finished simulations of: 20\n",
      "running rho: 0.8\n",
      "finished simulations of: 20\n",
      "running d0: 3\n",
      "running rho: 0\n",
      "finished simulations of: 20\n",
      "running rho: 0.5\n",
      "finished simulations of: 20\n",
      "running rho: 0.8\n",
      "finished simulations of: 20\n",
      "running d0: 4\n",
      "running rho: 0\n",
      "finished simulations of: 20\n",
      "running rho: 0.5\n",
      "finished simulations of: 20\n",
      "running rho: 0.8\n"
     ]
    }
   ],
   "source": [
    "from simulation_runner import runner as sim_runner\n",
    "import os\n",
    "import multiprocessing\n",
    "import itertools\n",
    "\n",
    "pool = multiprocessing.Pool(os.cpu_count())\n",
    "print(pool)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "# output = process_pool.starmap(f_sum, data)\n",
    "    results = {}\n",
    "    for d in simulation_settings:\n",
    "    #     d = 20\n",
    "        print('running d: {}'.format(d))\n",
    "        for di in simulation_settings[d]:\n",
    "            print('running d0: {}'.format(di))\n",
    "            for rho in [0, 0.5, 0.8]:\n",
    "                print('running rho: {}'.format(rho))\n",
    "                data = matrix_simulator(d, di, rho = rho)\n",
    "        #             print(data)\n",
    "                X_split = np.vsplit(data[0], 100)\n",
    "                y_split = np.split(data[1], 100)\n",
    "                theta_split = np.split(data[2], 100)\n",
    "                map_args = [(X,y,theta, 'poisson') for X,y,theta in zip(X_split,y_split,theta_split)]\n",
    "        #         print(map_args[0])\n",
    "                output = pool.starmap(sim_runner, map_args)\n",
    "                results[(d,di,rho)] = output\n",
    "                print('finished simulations of: {}'.format(d,di,rho))\n",
    "    #             output = pool.map(sim_runner, map_args)\n",
    "        print(\"Finished Pooling\")\n",
    "        \n",
    "#             print(zip(X_split,y_split,theta_split))\n",
    "#         result_forward = pool.starmap()\n",
    "#     print(d, simulation_settings[d])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c17d457c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'FWD': {'nll': 99.35713806900293,\n",
       "   'KL': 113327.65288423005,\n",
       "   'KL_theta_train': 107336.41232029392,\n",
       "   'KL_theta_test': 26939.28584059106,\n",
       "   'size': 2},\n",
       "  'LASSO': {'nll': 100.0,\n",
       "   'KL': 113795.92917122482,\n",
       "   'KL_theta_train': 108232.40193697,\n",
       "   'KL_theta_test': 26951.849003674917,\n",
       "   'size': 0},\n",
       "  'SLOPE': {'nll': 100.18733718367497,\n",
       "   'KL': 113982.65439084038,\n",
       "   'KL_theta_train': 108356.133129757,\n",
       "   'KL_theta_test': 26977.957944755915,\n",
       "   'size': 1}},\n",
       " {'FWD': {'nll': 100.69177586178331,\n",
       "   'KL': 115216.86434877384,\n",
       "   'KL_theta_train': 106929.0714084784,\n",
       "   'KL_theta_test': 27156.31033316313,\n",
       "   'size': 2},\n",
       "  'LASSO': {'nll': 100.08865434459278,\n",
       "   'KL': 119405.70620147133,\n",
       "   'KL_theta_train': 108014.48957653274,\n",
       "   'KL_theta_test': 27081.4053534604,\n",
       "   'size': 0},\n",
       "  'SLOPE': {'nll': 100.0,\n",
       "   'KL': 119068.93285354205,\n",
       "   'KL_theta_train': 108069.17305281028,\n",
       "   'KL_theta_test': 27014.03422566439,\n",
       "   'size': 0}},\n",
       " {'FWD': {'nll': 101.05881387225647,\n",
       "   'KL': 103489.24937908769,\n",
       "   'KL_theta_train': 107840.67266576277,\n",
       "   'KL_theta_test': 27390.05409703803,\n",
       "   'size': 2},\n",
       "  'LASSO': {'nll': 99.28958204746064,\n",
       "   'KL': 102327.5109708805,\n",
       "   'KL_theta_train': 108519.66887620019,\n",
       "   'KL_theta_test': 26676.442332091185,\n",
       "   'size': 1},\n",
       "  'SLOPE': {'nll': 100.0,\n",
       "   'KL': 103403.42327057893,\n",
       "   'KL_theta_train': 108358.6155593612,\n",
       "   'KL_theta_test': 27062.638876819758,\n",
       "   'size': 0}},\n",
       " {'FWD': {'nll': 100.34223065837857,\n",
       "   'KL': 133138.63776589124,\n",
       "   'KL_theta_train': 107480.14302964039,\n",
       "   'KL_theta_test': 26974.81728505946,\n",
       "   'size': 2},\n",
       "  'LASSO': {'nll': 99.34882814951918,\n",
       "   'KL': 132164.89666155854,\n",
       "   'KL_theta_train': 108147.50157240976,\n",
       "   'KL_theta_test': 26858.604165610603,\n",
       "   'size': 4},\n",
       "  'SLOPE': {'nll': 99.83914057335286,\n",
       "   'KL': 133598.00404549966,\n",
       "   'KL_theta_train': 108295.31641753785,\n",
       "   'KL_theta_test': 26967.170168701352,\n",
       "   'size': 1}},\n",
       " {'FWD': {'nll': 99.68414806620113,\n",
       "   'KL': 66596.59346782594,\n",
       "   'KL_theta_train': 107992.6474153721,\n",
       "   'KL_theta_test': 27211.33275942179,\n",
       "   'size': 2},\n",
       "  'LASSO': {'nll': 100.22496175166584,\n",
       "   'KL': 66588.78041697117,\n",
       "   'KL_theta_train': 108187.73834500433,\n",
       "   'KL_theta_test': 26992.421773154623,\n",
       "   'size': 3},\n",
       "  'SLOPE': {'nll': 100.0,\n",
       "   'KL': 66512.52875749629,\n",
       "   'KL_theta_train': 108069.44340709964,\n",
       "   'KL_theta_test': 27005.10002301925,\n",
       "   'size': 0}},\n",
       " {'FWD': {'nll': 108.36042075836171,\n",
       "   'KL': 178536.5865674263,\n",
       "   'KL_theta_train': 108711.40179349596,\n",
       "   'KL_theta_test': 26645.141356578282,\n",
       "   'size': 3},\n",
       "  'LASSO': {'nll': 99.72271656931198,\n",
       "   'KL': 176353.89567196005,\n",
       "   'KL_theta_train': 108264.67599544887,\n",
       "   'KL_theta_test': 27061.024883939514,\n",
       "   'size': 0},\n",
       "  'SLOPE': {'nll': 99.71389652632746,\n",
       "   'KL': 176353.89957808028,\n",
       "   'KL_theta_train': 108269.62277744099,\n",
       "   'KL_theta_test': 27062.049725717112,\n",
       "   'size': 0}},\n",
       " {'FWD': {'nll': 100.80527775578274,\n",
       "   'KL': 166944.88554835905,\n",
       "   'KL_theta_train': 108035.15929076776,\n",
       "   'KL_theta_test': 26972.153853107204,\n",
       "   'size': 2},\n",
       "  'LASSO': {'nll': 100.35820021269939,\n",
       "   'KL': 166699.76505106475,\n",
       "   'KL_theta_train': 108187.4586345175,\n",
       "   'KL_theta_test': 26945.719804090917,\n",
       "   'size': 2},\n",
       "  'SLOPE': {'nll': 101.18081361114675,\n",
       "   'KL': 167534.83396000494,\n",
       "   'KL_theta_train': 108848.88765558254,\n",
       "   'KL_theta_test': 27100.860554767238,\n",
       "   'size': 1}},\n",
       " {'FWD': {'nll': 99.99340909169356,\n",
       "   'KL': 79488.13399629005,\n",
       "   'KL_theta_train': 108130.15304333535,\n",
       "   'KL_theta_test': 27345.135833415803,\n",
       "   'size': 2},\n",
       "  'LASSO': {'nll': 103.96532897928307,\n",
       "   'KL': 82010.93697217997,\n",
       "   'KL_theta_train': 108132.36814063607,\n",
       "   'KL_theta_test': 28309.423413540262,\n",
       "   'size': 5},\n",
       "  'SLOPE': {'nll': 99.97463024381011,\n",
       "   'KL': 79287.60667399438,\n",
       "   'KL_theta_train': 108429.22158831435,\n",
       "   'KL_theta_test': 27111.209668194602,\n",
       "   'size': 1}}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results[(d, di, rho)] = [res for res in output]\n",
    "# results\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4697b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "301bba9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-5.78391609e-03, -4.03495220e-03, -4.18974836e-03, ...,\n",
       "          2.66413076e-03,  1.14657787e-03,  3.98668167e-03],\n",
       "        [ 2.08829305e-03,  3.11895185e-03,  2.12775132e-03, ...,\n",
       "         -1.07492684e-02,  2.94142482e-04, -2.96663166e-04],\n",
       "        [-1.63103498e-03,  5.23481844e-03, -9.67172344e-03, ...,\n",
       "          4.15401949e-03,  1.73961241e-04,  1.17674197e-03],\n",
       "        ...,\n",
       "        [ 2.53987048e-03,  1.80619388e-04, -7.26024073e-04, ...,\n",
       "         -1.10541895e-02, -5.31152890e-03, -5.14544308e-03],\n",
       "        [-1.46999783e-03,  2.38073217e-05,  5.94246903e-03, ...,\n",
       "          1.32458038e-04, -1.77938026e-03,  4.90695259e-03],\n",
       "        [-3.88378712e-03,  1.78927393e-03,  2.50552796e-04, ...,\n",
       "          2.00161648e-03,  4.07938619e-03, -8.57777341e-04]]),\n",
       " array([2, 4, 2, ..., 0, 1, 0]),\n",
       " array([1.00306099, 1.00326202, 0.9991098 , ..., 1.0004237 , 0.99905366,\n",
       "        0.99958739]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 20\n",
    "d0 = 5\n",
    "matrix_simulator(20, 5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52069446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
